{"people":{"Zijie J. Wang":{"url":"https://zijie.wang","isMe":true},"Anthony Gitter":{"url":"https://www.biostat.wisc.edu/~gitter"},"Melissa C. Skala":{"url":"https://morgridge.org/research/medical-engineering/optical-microscopy"},"Alex J. Walsh":{"url":"https://qoil.engr.tamu.edu"},"Duen Horng (Polo) Chau":{"url":"https://www.cc.gatech.edu/~dchau"},"Polo Chau":{"url":"https://www.cc.gatech.edu/~dchau"},"Fred Hohman":{"url":"https://fredhohman.com"},"Minsuk Kahng":{"url":"https://minsuk.com"},"Haekyu Park":{"url":"https://haekyu.com"},"Nilaksh Das":{"url":"https://nilakshdas.com"},"Robert Turko":{"url":"https://www.linkedin.com/in/robert-turko/"},"Omar Shaikh":{"url":"http://oshaikh.com/"},"Michael Gleicher":{"url":"http://pages.cs.wisc.edu/~gleicher/"},"Yu Hen Hu":{"url":"http://homepages.cae.wisc.edu/~hu/"},"Tiffany M. Heaster":{"url":"https://morgridge.org/profile/tiffany-heaster/"},"Quan Yin":{"url":"https://www.linkedin.com/in/quan-yin/"},"Emily Rogers":{"url":"https://www.linkedin.com/in/emily-rogers-1a828598"},"Robert Firstman":{"url":"https://www.robfirstman.com/"},"Scott Freitas":{"url":"https://www.scottfreitas.com/"},"Shang-Tse Chen":{"url":"https://www.cc.gatech.edu/~schen351/"},"Jon Saad-Falcon":{"url":"https://www.linkedin.com/in/jonsaadfalcon/"},"Austin P. Wright":{"url":"https://austinpwright.com/"},"Sasha Richardson":{"url":"https://www.linkedin.com/in/sasha-richardson/"},"Siwei Li":{"url":"https://rsli.github.io/"},"Zhiyan Zhou":{"url":"https://www.linkedin.com/in/frank-zhou-b19515159/"},"Anish Upadhayay":{"url":"https://github.com/aupadhayay3"},"Susanta Routray":{"url":"https://www.linkedin.com/in/susantaroutray/"},"Matthew Hull":{"url":"https://www.linkedin.com/in/mdhull/"},"Liang Gou":{"url":"https://www.linkedin.com/in/lianggou/"},"Grace Guo":{"url":"https://gracegsy.github.io/"},"Fabian Sperrle":{"url":"https://www.vis.uni-konstanz.de/mitglieder/sperrle/"},"Mennatallah El-Assady":{"url":"https://el-assady.com/"},"Alex Endert":{"url":"https://va.gatech.edu/endert/"},"Daniel Keim":{"url":"https://www.vis.uni-konstanz.de/en/members/keim"}},"education":[{"school":"Georgia Institute of Technology","schoolURL":"https://www.gatech.edu","place":"Atlanta, GA","timeStart":"Aug. 2019","timeEnd":"Present","descriptions":["Ph.D. in Machine Learning"],"advisors":["Duen Horng (Polo) Chau"]},{"school":"University of Wisconsin–Madison","schoolURL":"https://www.wisc.edu","place":"Madison, WI","timeStart":"Sept. 2015","timeEnd":"May 2019","descriptions":["Bachelor of Science (B.S.), GPA: 3.95/4.00","Majors: Computer Sciences (Honor), Statistics (Honor), Mathematics"],"advisors":["Anthony Gitter","Michael Gleicher","Yu Hen Hu"],"thesis":{"title":"Classifying T Cell Activity with Convolutional Neural Networks","file":"pdf/undergrad-thesis.pdf"}}],"experienceAcademic":[{"institution":"Georgia Institute of Technology","place":"Atlanta, GA","position":"Ph.D. Researcher","group":"School of Computational Science and Engineering","timeStart":"Aug. 2019","timeEnd":"Present","mentors":["Duen Horng (Polo) Chau"],"description":"Member of the Polo Club of Data Science where we innovate scalable, interactive, and interpretable tools that amplify human's ability to understand and interact with billion-scale data and machine learning models.\n","institutionURL":"https://www.gatech.edu","groupURL":"https://cse.gatech.eduu"},{"institution":"Morgridge Institute for Research","place":"Madison, WI","position":"Undergraduate Researcher","group":"John W. and Jeanne M. Rowe Center for Research in Virology","timeStart":"Dec. 2017","timeEnd":"Aug. 2019","mentors":["Anthony Gitter"],"description":"Classify T-cell and breast cancer cell types using fluorescent images with machine learning classifiers with a gradient of complexity. Interpre feature representations of each classifiers. Analyze about 1 million 5-channel cell-painting images of bone tumor cells. Explore latent space between image space and chemical molecule space.\n","institutionURL":"https://morgridge.org","groupURL":"https://morgridge.org/research/virology/"},{"institution":"University of Wisconsin–Madison","place":"Madison, WI","position":"Undergraduate Researcher","group":"Department of Computer Sciences","timeStart":"Dec. 2018","timeEnd":"June 2019","mentors":["Michael Gleicher"],"description":"Design and develop a visual analytics tool for recommender system resaerchers. Interactively visualized user-item rating matrix with statistics-conditioned sub-sampling to spot abnormal ratings and predictions.\n","institutionURL":"https://www.wisc.edu","groupURL":"https://www.cs.wisc.edu"},{"institution":"University of Wisconsin–Madison","place":"Madison, WI","position":"Research Assistant","group":"Electrical & Computer Engineering","timeStart":"Feb. 2017","timeEnd":"Dec. 2017","mentors":["Yu Hen Hu"],"description":"Study how to track car driver’s head position and orientation from low-qualitytraffic video. Develop semi-automatic video annotation software with Viola-Jones frontal facedetector for training object tracking algorithms. Implement real-time face tracking algorithms on iOS devices. Train a facial reenactment model using GANs and port it to iOS device.\n","institutionURL":"https://www.wisc.edu","groupURL":"https://www.engr.wisc.edu/department/electrical-computer-engineering/"}],"experienceIndustry":[{"institution":"Bosch Research","place":"Sunnyvale, CA","position":"Research Intern","group":"Human–Machine Interaction","timeStart":"June 2020","timeEnd":"Aug. 2020","mentors":["Liang Gou"],"description":"Research and develop explainable machine learning models and visual analytics solutions to help autonomous driving domain experts understand and control adversarial case generation for object recognition.\n","institutionURL":"https://www.bosch.us/our-company/bosch-in-the-usa/sunnyvale/","groupURL":"https://www.bosch.us/our-company/bosch-in-the-usa/sunnyvale/"}],"publication":[{"id":"cnn-explainer","title":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization","authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"venue":"IEEE Transactions on Visualization and Computer Graphics","venueURL":"https://www.computer.org/csdl/journal/tg","venueShort":"TVCG","location":"Salt Lake City, UT, USA","year":2021,"url":"/papers/cnn-explainer","pdf":"https://arxiv.org/abs/2004.15004","repo":"poloclub/cnn-explainer","showStar":true,"awards":[{"name":"Top of GitHub Trending","url":"https://web.archive.org/web/20200505121955/https://github.com/trending"}],"video":"https://youtu.be/HnWIHWFbuUQ","demo":"https://poloclub.github.io/cnn-explainer/","abstract":"Deep learning's great success motivates many practitioners and students to\nlearn about this exciting technology. However, it is often challenging for\nbeginners to take their first step due to the complexity of understanding\nand applying deep learning. We present CNN Explainer, an interactive\nvisualization tool designed for non-experts to learn and examine\nconvolutional neural networks (CNNs), a foundational deep learning model\narchitecture. Our tool addresses key challenges that novices face while\nlearning about CNNs, which we identify from interviews with instructors and\na survey with past students. CNN Explainer tightly integrates a model\noverview that summarizes a CNN's structure, and on-demand, dynamic visual\nexplanation views that help users understand the underlying components of\nCNNs. Through smooth transitions across levels of abstraction, our tool\nenables users to inspect the interplay between low-level mathematical\noperations and high-level model structures. A qualitative user study shows\nthat CNN Explainer helps users more easily understand the inner workings of\nCNNs, and is engaging and enjoyable to use. We also derive design lessons\nfrom our study. Developed using modern web technologies, CNN Explainer runs\nlocally in users' web browsers without the need for installation or\nspecialized hardware, broadening the public's education access to modern\ndeep learning techniques.","crownCaption":"With CNN Explainer, learners can visually examine how Convolutional Neural\nNetworks (CNNs) transform input images into classification predictions\n(e.g., predicting espresso for an image of a coffee cup), and interactively\nlearn about their underlying mathematical operations. In this example, a\nlearner uses CNN Explainer to understand how convolutional layers work\nthrough three tightly integrated views, each explaining the convolutional\nprocess in increasing levels of detail. (A) The Overview visualizes a CNN\narchitecture where each neuron is encoded as a square with a heatmap\nrepresenting the neuron’s output, and each edge connects the neuron with its\ncorresponding inputs and outputs. (B) Clicking a neuron reveals how its\nactivations are computed by the previous layer’s neurons, displaying the\noften-overlooked intermediate computation through animations of sliding\nkernels. (C) The Convolutional Interactive Formula View allows users to\ninteractively inspect the underlying mathematics of the dot-product\noperation core to convolution, through hovering the 3×3 kernel over the\ninput, and interactively studying the corresponding output. For clarity,\nvisibility of Overview and annotation text is improved, and the overlay is\nre-positioned.","bibtex":"@article{wangCNNExplainerLearning2021,\n  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},\n  shorttitle = {{{CNN Explainer}}},\n  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  journal={IEEE Transactions on Visualization and Computer Graphics (TVCG)},\n  publisher={IEEE},\n  year={2021},\n}"},{"id":"bluff","title":"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks","authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"equals":["Nilaksh Das","Haekyu Park"],"venue":"IEEE Visualization Conference","venueURL":"http://ieeevis.org/year/2020/welcome","venueShort":"VIS","location":"Salt Lake City, UT, USA","year":2020,"url":"/papers/bluff","pdf":"https://arxiv.org/abs/2009.02608","repo":"poloclub/bluff","showStar":false,"demo":"https://poloclub.github.io/bluff/","abstract":"Deep neural networks (DNNs) are now commonly used in many domains. However,\nthey are vulnerable to adversarial attacks: carefully crafted perturbations\non data inputs that can fool a model into making incorrect predictions.\nDespite significant research on developing DNN attack and defense\ntechniques, people still lack an understanding of how such attacks penetrate\na model's internals. We present Bluff, an interactive system for\nvisualizing, characterizing, and deciphering adversarial attacks on\nvision-based neural networks. Bluff allows people to flexibly visualize and\ncompare the activation pathways for benign and attacked images, revealing\nmechanisms that adversarial attacks employ to inflict harm on a model. Bluff\nis open-sourced and runs in modern web browsers. ","crownCaption":"With Bluff, users interactively visualize how adversarial attacks penetrate\na deep neural network to induce incorrect outcomes. Here, a user inspects\nwhy Inception V1 misclassifies adversarial giant panda images, crafted by\nthe Projected Gradient Descent (PGD) attack, as armadillo. PGD successfully\nperturbed pixels to induce the “brown bird” feature, an appearance more\nlikely shared by an armadillo (small, roundish, brown body) than a panda,\nactivating more features that contribute to the armadillo (mis)classification\n(e.g., “scales,” “bumps,” “mesh”). The adversarial pathways, formed by these\nneurons and their connections, overwhelm the benign panda pathways and lead\nto the ultimate misclassification. (A) Control Side bar allows users to\nspecify what data is to be included and highlighted. (B) Graph Summary View\nvisualizes pathways most activated or changed by an attack as a network\ngraph of neurons (each labeled by the channel ID in its layer) and their\nconnections. When hovering over a neuron, (C) Detail View displays its\nfeature visualization, representative dataset examples, and activation\npatterns over attack strengths.","bibtex":"@article{dasBluffInteractivelyDeciphering2020,\n  title={Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks},\n  author={Das, Nilaksh and Park, Haekyu and Wang, Zijie J and Hohman, Fred and Firstman, Robert and Rogers, Emily and Chau, Duen Horng},\n  booktitle={IEEE Visualization Conference (VIS)},\n  publisher={IEEE},\n  year={2020}\n}"},{"id":"ai-guideline","title":"A Comparative Analysis of Industry Human-AI Interaction Guidelines","authors":["Austin P. Wright","Zijie J. Wang","Haekyu Park","Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel Keim","Duen Horng (Polo) Chau"],"venue":"IEEE Visualization Conference, Workshop on Trust and Expertise in Visual Analytics","venueURL":"https://trexvis.github.io/Workshop2020/","venueShort":"TREX","location":"Salt Lake City, UT, USA","year":2020,"url":"/papers/ai-guideline","pdf":"https://arxiv.org/abs/2010.11761","repo":"APWright/AI-Open-Guidelines","showStar":false,"demo":"https://ai-open-guidelines.readthedocs.io/en/latest/","crownMaxHeight":"750px","crownShowBorder":true,"abstract":"With the recent release of AI interaction guidelines from Apple, Google, and\nMicrosoft, there is clearly interest in understanding the best practices in\nhuman-AI interaction. However, industry standards are not determined by a\nsingle company, but rather by the synthesis of knowledge from the whole\ncommunity. We have surveyed all of the design guidelines from each of these\nmajor companies and developed a single, unified structure of guidelines,\ngiving developers a centralized reference. We have then used this framework\nto compare each of the surveyed companies to find differences in areas of\nemphasis. Finally, we encourage people to contribute additional guidelines\nfrom other companies, academia, or individuals, to provide an open and\nextensible reference of AI design guidelines at <a\nhref='https://ai-open-guidelines.readthedocs.io'>https://ai-open-guidelines.readthedocs.io</a>.    ","crownCaption":"Unified Guideline Structure. The inner ring consists of the higher level\ncategorizations, and further sub-categorizations developed during the\naffinity diagram process are shown concentrically. The outermost rays\nconsist of the specific guidelines colored based on their\ncategorizations. Further references on each guideline and its corresponding\ncategorization and source document can be found in the appendix.","bibtex":"@article{wrightComparativeAnalysisIndustry2020,\n  title = {A {{Comparative Analysis}} of {{Industry Human}}-{{AI Interaction Guidelines}}},\n  author = {Wright, Austin P. and Wang, Zijie J. and Park, Haekyu and Guo, Grace and Sperrle, Fabian and {El-Assady}, Mennatallah and Endert, Alex and Keim, Daniel and Chau, Duen Horng},\n  year = {2020},\n  month = oct,\n  eprint = {2010.11761},\n  journal = {arXiv:2010.11761}\n}"},{"id":"people-map","title":"Mapping Researchers with PeopleMap","authors":["Jon Saad-Falcon","Omar Shaikh","Zijie J. Wang","Austin P. Wright","Sasha Richardson","Duen Horng (Polo) Chau"],"venue":"Poster, IEEE Visualization Conference","venueURL":"http://ieeevis.org/year/2020/welcome","venueShort":"VIS","year":2020,"location":"Salt Lake City, UT, USA","url":"/papers/people-map","pdf":"https://arxiv.org/abs/2009.00091","repo":"poloclub/people-map","type":"arxiv","demo":"https://poloclub.github.io/people-map/ideas/","showStar":true,"awards":[{"name":"Best Poster Research Award, Honorable Mention","url":"https://virtual.ieeevis.org/awards.html"}],"abstract":"Discovering research expertise at institutions can be a difficult task.\nManually curated university directories easily become out of date and they\noften lack the information necessary for understanding a researcher's\ninterests and past work, making it harder to explore the diversity of\nresearch at an institution and identify research talents. This results in\nlost opportunities for both internal and external entities to discover new\nconnections and nurture research collaboration. To solve this problem, we\nhave developed PeopleMap, the first interactive, open-source, web-based tool\nthat visually \"maps out\" researchers based on their research interests and\npublications by leveraging embeddings generated by natural language\nprocessing (NLP) techniques. PeopleMap provides a new engaging way for\ninstitutions to summarize their research talents and for people to discover\nnew connections. The platform is developed with ease-of-use and\nsustainability in mind. Using only researchers' Google Scholar profiles as\ninput, PeopleMap can be readily adopted by any institution using its\npublicly-accessible repository and detailed documentation.","crownCaption":"PeopleMap visually maps out researchers based on their research interests\nand publications. Here, a PeopleMap user is exploring the research topics of\nthe faculty members at the Institute of Data Engineering and Science (IDEaS)\nat Georgia Tech (https://poloclub.github.io/people-map/ideas/) A. Map View\nvisualizes the embedding of researchers generated using their research\ntopics and publication data, with each dot representing a researcher. B.\nResearch Query allows users to search for researchers and query areas of\nstudy, allowing the user to both locate specific individuals and see the\nresearchers most associated with a queried field in the Map View. C.\nResearcher View shows the detailed information (e.g., affiliation,\ncitations, interests) of a researcher highlighted in Map View. D. Control\nPanel allows users to adjust the hyperparameters of the Map View\nvisualization (e.g., show research names and cluster information).","bibtex":"@article{saad-falconMappingResearchersPeopleMap2020,\n  title = {Mapping {{Researchers}} with {{PeopleMap}}},\n  author = {{Saad-Falcon}, Jon and Shaikh, Omar and Wang, Zijie J. and Wright, Austin P. and Richardson, Sasha and Chau, Duen Horng},\n  year = {2020},\n  journal = {Poster, IEEE Visualization Conference (VIS)}\n}"},{"id":"argo-lite","title":"Argo Lite: Open-Source Interactive Graph Exploration and Visualization in Browsers","authors":["Siwei Li","Zhiyan Zhou","Anish Upadhayay","Omar Shaikh","Scott Freitas","Haekyu Park","Zijie J. Wang","Susanta Routray","Matthew Hull","Duen Horng (Polo) Chau"],"venue":"The Conference on Information and Knowledge Management","venueShort":"CIKM","venueURL":"https://www.cikm2020.org/","year":2020,"location":"Galway, Ireland","url":"/papers/argo-lite","pdf":"https://arxiv.org/abs/2008.11844","repo":"poloclub/argo-graph-lite","showStar":true,"type":"arxiv","demo":"https://poloclub.github.io/argo-graph-lite/","abstract":"Graph data have become increasingly common. Visualizing them helps people\nbetter understand relations among entities. Unfortunately, existing graph\nvisualization tools are primarily designed for single-person desktop use,\noffering limited support for interactive web-based exploration and online\ncollaborative analysis. To address these issues, we have developed Argo\nLite, a new in-browser interactive graph exploration and visualization tool.\nArgo Lite enables users to publish and share interactive graph\nvisualizations as URLs and embedded web widgets. Users can explore graphs\nincrementally by adding more related nodes, such as highly cited papers\ncited by or citing a paper of interest in a citation network. Argo Lite\nworks across devices and platforms, leveraging WebGL for high-performance\nrendering. Argo Lite has been used by over 1,000 students at Georgia Tech's\nData and Visual Analytics class. Argo Lite may serve as a valuable\nopen-source tool for advancing multiple CIKM research areas, from data\npresentation, to interfaces for information systems and more.","crownCaption":"Argo Lite visualizing a citation network of recent COVID-19 publications.\nArgo Lite users can explore graphs incrementally by adding more\nrelated papers (e.g., highly cited papers cited by or citing a paper of\ninterest) to the visualization. Using WebGL for high-performance\ncross-platform graph rendering, Argo Lite runs in all modern web\nbrowsers without requiring any installation.","bibtex":"@article{liArgoLiteOpenSource2020,\n  title={Argo Lite: Open-Source Interactive Graph Exploration and Visualization in Browsers},\n  author={Li, Siwei and Zhou, Zhiyan and Upadhayay, Anish and Shaikh, Omar and Freitas, Scott and Park, Haekyu and Wang, Zijie J and Routray, Susanta and Hull, Matthew and Chau, Duen Horng},\n  booktitle={Proceedings of the International Conference on Information and Knowledge Management},\n  year={2020},\n  organization={ACM}\n}"},{"id":"unmask","title":"UnMask: Adversarial Detection and Defense Through Robust Feature Alignment","authors":["Scott Freitas","Shang-Tse Chen","Zijie J. Wang","Duen Horng (Polo) Chau"],"venue":"IEEE International Conference on Big Data","venueShort":"BigData","venueURL":"https://bigdataieee.org/BigData2020/","year":2020,"location":"Los Angeles, CA, USA","url":"/papers/unmask","pdf":"https://arxiv.org/abs/2002.09576","repo":"unmaskd/unmask","type":"arxiv","abstract":"Deep learning models are being integrated into a wide range of high-impact,\nsecurity-critical systems, from self-driving cars to medical diagnosis.\nHowever, recent research has demonstrated that many of these deep learning\narchitectures are vulnerable to adversarial attacks--highlighting the vital\nneed for defensive techniques to detect and mitigate these attacks before\nthey occur. To combat these adversarial attacks, we developed UnMask, an\nadversarial detection and defense framework based on robust feature\nalignment. The core idea behind UnMask is to protect these models by\nverifying that an image's predicted class (\"bird\") contains the expected\nrobust features (e.g., beak, wings, eyes). For example, if an image is\nclassified as \"bird\", but the extracted features are wheel, saddle and\nframe, the model may be under attack. UnMask detects such attacks and\ndefends the model by rectifying the misclassification, re-classifying the\nimage based on its robust features. Our extensive evaluation shows that\nUnMask (1) detects up to 96.75% of attacks, with a false positive rate of\n9.66% and (2) defends the model by correctly classifying up to 93% of\nadversarial images produced by the current strongest attack, Projected\nGradient Descent, in the gray-box setting. UnMask provides significantly\nbetter protection than adversarial training across 8 attack vectors,\naveraging 31.18% higher accuracy. Our proposed method is architecture\nagnostic and fast. We open source the code repository and data with this\npaper: https://github.com/unmaskd/unmask. ","crownShowBorder":true,"crownCaption":"UnMask Framework Overview. UnMask combats adversarial attacks (in\nred) through extracting robust features from an image (“Bicycle” at top), and\ncomparing them to expected features of the classification (“Bird” at bottom)\nfrom the unprotected model. Low feature overlap signals an\nattack. UnMask rectifies misclassification using the image’s extracted\nfeatures. Our approach detects 96.75% of gray-box attacks (at 9.66% false\npositive rate) and defends the model by correctly classifying up to 93%\nof adversarial images crafted by Projected Gradient Descent (PGD).","bibtex":"@article{freitasUnMaskAdversarialDetection2020,\n  title = {{{UnMask}}: {{Adversarial Detection}} and {{Defense Through Robust Feature Alignment}}},\n  shorttitle = {{{UnMask}}},\n  author = {Freitas, Scott and Chen, Shang-Tse and Wang, Zijie J. and Chau, Duen Horng},\n  year = {2020},\n  archivePrefix = {arXiv},\n  eprint = {2002.09576},\n  eprinttype = {arxiv},\n  journal = {arXiv:2002.09576}\n}"},{"id":"massif","title":"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning","authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"equals":["Nilaksh Das","Haekyu Park"],"venue":"Extended Abstracts on ACM Human Factors in Computing Systems","venueURL":"https://dl.acm.org/doi/abs/10.1145/3334480.3382977","venueShort":"CHI","location":"Honolulu, HI, USA","year":2020,"url":"/papers/massif","pdf":"https://arxiv.org/abs/2001.07769","type":"arxiv","abstract":"Deep neural networks (DNNs) are increasingly powering high-stakes\napplications such as autonomous cars and healthcare; however, DNNs are often\ntreated as \"black boxes\" in such applications. Recent research has also\nrevealed that DNNs are highly vulnerable to adversarial attacks, raising\nserious concerns over deploying DNNs in the real world. To overcome these\ndeficiencies, we are developing Massif, an interactive tool for deciphering\nadversarial attacks. Massif identifies and interactively visualizes neurons\nand their connections inside a DNN that are strongly activated or suppressed\nby an adversarial attack. Massif provides both a high-level, interpretable\noverview of the effect of an attack on a DNN, and a low-level, detailed\ndescription of the affected neurons. Massif's tightly coupled views help\npeople better understand which input features are most vulnerable and\nimportant for correct predictions.","crownCaption":"The MASSIF interface. A user Hailey is studying the targeted Fast Gradient\nMethod (FGM) attack performed on the InceptionV1 model. Using the control\npanel (A), she selects “giant panda” as the benign class and “armadillo” as\nthe attack target class. MASSIF generates an attribution graph (B), which\nshows Hailey the neurons within the network that are suppressed in the\nattacked images (B1, blue), shared by both benign and attacked images (B2,\npurple), and emphasized only in the attacked images (B3, orange). Each\nneuron is represented by a node and its feature visualization (C). Hovering\nover any neuron displays example dataset patches that maximally activate\nthe neuron, providing stronger evidence for what a neuron has learned to\ndetect. Hovering over a neuron also highlights its most influential\nconnections from the previous layer (D), allowing Hailey to determine where\nin the network the prediction diverges from the benign class to the attacked\nclass.","bibtex":"@inproceedings{das2020massif,\n  title={Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning},\n  author={Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  booktitle={Proceedings of the 2020 CHI Conference Extended Abstracts on Human Factors in Computing Systems},\n  publisher={ACM},\n  year={2020}\n}"},{"id":"cnn-101","title":"CNN 101: Interactive Visual Learning for Convolutional Neural Networks","authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"venue":"Extended Abstracts on ACM Human Factors in Computing Systems","venueShort":"CHI","location":"Honolulu, HI, USA","venueURL":"https://dl.acm.org/doi/abs/10.1145/3334480.3382899","year":2020,"url":"/papers/cnn-101","pdf":"https://arxiv.org/abs/2001.02004","type":"arxiv","video":"https://youtu.be/g082-zitM7s","crownCaption":"The Overview (A) visualizes activation maps of all neurons as heatmaps\nconnected with edges. When user clicks a convolutional neuron in (A), the\nview transitions to the Convolutional Intermediate View (A=>B).The Flatten\nIntermediate View appears when an output neuron is selected instead\n(A=>C). (B) demonstrates the relationship between selected convolutional\nneuron and its previous layer. (B) transitions to the Detail View which illustrates\nthe convolution operation on selected input neuron (B=>D). (C) explains\nthe flatten layer between the second last layer and output layer.","abstract":"The success of deep learning solving previously-thought hard problems has\ninspired many non-experts to learn and understand this exciting technology.\nHowever, it is often challenging for learners to take the first steps due to\nthe complexity of deep learning models. We present our ongoing work, CNN\n101, an interactive visualization system for explaining and teaching\nconvolutional neural networks. Through tightly integrated interactive views,\nCNN 101 offers both overview and detailed descriptions of how a model works.\nBuilt using modern web technologies, CNN 101 runs locally in users' web\nbrowsers without requiring specialized hardware, broadening the public's\neducation access to modern deep learning techniques. ","bibtex":"@inproceedings{wangCNN101Interactive2020,\n  title = {{{CNN}} 101: {{Interactive}} Visual Learning for Convolutional Neural Networks},\n  booktitle = {Extended Abstracts of the 2020 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  year = {2020},\n  publisher = {{ACM}},\n  place = {{Honolulu, HI, USA}}\n}"},{"id":"t-cell","title":"Classifying T cell activity in autofluorescence intensity images with convolutional neural networks","authors":["Zijie J. Wang","Alex J. Walsh","Melissa C. Skala","Anthony Gitter"],"venue":"Journal of Biophotonics","venueShort":"J. Biophotonics","venueURL":"https://onlinelibrary.wiley.com/journal/18640648","year":2019,"url":"/papers/t-cell","pdf":"https://onlinelibrary.wiley.com/doi/epdf/10.1002/jbio.201960050","slides":"/slides/honor_thesis_symposium_2019.pdf","repo":"gitter-lab/t-cell-classification","showStar":"flase","data":"https://doi.org/10.5281/zenodo.2640835","type":"journal","crownShowBorder":true,"crownCaption":"Our T cell image data processing workflow.","abstract":"The importance of T cells in immunotherapy has motivated developing\ntechnologies to better characterize T cells and improve therapeutic\nefficacy. One specific objective is assessing antigen-induced T cell\nactivation because only functionally active T cells are capable of killing\nthe desired targets. Autofluorescence imaging can distinguish T cell\nactivity states of individual cells in a non-destructive manner by detecting\nendogenous changes in metabolic co-enzymes such as NAD(P)H. However,\nrecognizing robust patterns of T cell activity is computationally\nchallenging in the absence of exogenous labels or information-rich\nautofluorescence lifetime measurements. We demonstrate that advanced machine\nlearning can accurately classify T cell activity from NAD(P)H intensity\nimages and that those image-based signatures transfer across human donors.\nUsing a dataset of 8,260 cropped single-cell images from six donors, we\nmeticulously evaluate multiple machine learning models. These range from\ntraditional models that represent images using summary statistics or extract\nimage features with CellProfiler to deep convolutional neural networks\n(CNNs) pre-trained on general non-biological images. Adapting pre-trained\nCNNs for the T cell activity classification task provides substantially\nbetter performance than traditional models or a simple CNN trained with the\nautofluorescence images alone. Visualizing the images with dimension\nreduction provides intuition into why the CNNs achieve higher accuracy than\nother approaches. However, we observe that fine-tuning all layers of the\npre-trained CNN does not provide a classification performance boost\ncommensurate with the additional computational cost. Our software detailing\nour image processing and model training pipeline is available as Jupyter\nnotebooks at https://github.com/gitter-lab/t-cell-classification.","bibtex":"@article{wang_classifying_2019,\n  title = {Classifying {T} cell activity in autofluorescence intensity images with convolutional neural networks},\n  issn = {1864-063X, 1864-0648},\n  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jbio.201960050},\n  doi = {10.1002/jbio.201960050},\n  language = {en},\n  urldate = {2020-01-12},\n  journal = {Journal of Biophotonics},\n  author = {Wang, Zijie J. and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  month = dec,\n  year = {2019}\n}"},{"id":"t-cell-poster","title":"Classifying T cell activity with convolutional neural networks","authors":["Zijie J. Wang","Alex J. Walsh","Melissa C. Skala","Anthony Gitter"],"venue":"International Society for Computational Biology Great Lakes Bioinformatics Conference","venueShort":"ISCB GLBIO","venueURL":"https://www.iscb.org/glbio2019","location":"Madison, WI, USA","year":2019,"url":"/papers/t-cell-poster","pdf":"/pdf/19-tcell-glbio.pdf","code":"https://github.com/gitter-lab/t-cell-classification","type":"poster","crownShowBorder":true,"crownCaption":"Poster presented at the International Society for Computational Biology\nGreat Lakes Bioinformatics Conference (ISCB GLBIO).","abstract":"T cell activity state is an important component of immunotherapy efficacy in\nclinical cancer treatment. However, current image-based activity profiling\nmethods destroy cells and require exogenous contrast agents, making them\nunsuitable for clinical applications. In this study, we use non-destructive,\nT cell autofluorescence microscopy images to measure NAD(P)H intensity and\nclassify individual T cells as activated or quiescent. We assess five\nmachine learning methods of increasing complexity, ranging from linear\nclassifiers to deep convolutional neural networks pre-trained on generic\nimages. To evaluate these models and determine whether they are accurate\nacross different human T cell donors, we designed a meticulous nested\ncross-validation scheme to tune and test each model. A retrained\nconvolutional neural network, the most advanced model, achieved an average\naccuracy of 91.4% when classifying quiescent and activated T cells.\nImportantly, it gave 98% accuracy on an independent donor that was held out\nuntil all aspects of the training and tuning procedures were finalized. This\nshows that autofluorescence microscopy with a state-of-the-art image\nclassification algorithm is a powerful tool for label-free and\nnon-destructive assessment of T cell activity state, even when only NAD(P)H\nintensity is provided as the input feature. In addition, our high-throughput\nhyperparameter selection results give empirical insights on practical deep\nlearning deployment with microscopy image data. Similarly, the model\ncomparisons examined the tradeoff between performance and model complexity,\nwhich provides alternative methods that are suitable when computing resource\nare limited. We observe that retraining more layers in a pre-trained\nconvolutional neural network does not bring performance improvements that\njustify the high computational costs. Finally, we are preparing all of our\ncode in Jupyter notebooks with reproducible examples of image processing and\nclassification. These comprehensive notebooks serve as an instructional tool\nfor readers who are not familiar with machine learning application on\nmicroscopy images.","bibtex":"@inproceedings{wang_classifying_poster_2019,\n  title = {Classifying {T} cell activity with convolutional neural networks},\n  language = {en},\n  conference = {International Society for Computational Biology Great Lakes Bioinformatics Conference},\n  author = {Wang, Zijie J. and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  year = {2019}\n}"},{"id":"breast-poster","title":"Using Transfer Learning to Classify Breast Cancer Cells with Fluorescence Imaging","authors":["Zijie J. Wang","Tiffany M. Heaster","Quan Yin","Alex J. Walsh","Melissa C. Skala","Anthony Gitter"],"venue":"University of Wisconsin–Madison Undergraduate Symposium","venue-short":"","venueURL":"https://ugradsymposium.wisc.edu","year":2018,"url":"/papers/breast-poster","pdf":"/pdf/18-breast-symposium.pdf","type":"poster","abstract":"Studying tumor heterogeneity by analyzing protein or gene expression levels\nover thousands of cells is very challenging. In this project, we instead use\na transfer learning approach to classify cancer cell types solely based on\nfluorescence imaging. We used images of two types of breast cancer cell\nlines – MDA-MB-231 and SKBr3 – to partially retrain a deep convolutional\nneural network Inception v3, which was pre-trained on 10 million natural\nimages with over 400 categories. We hypothesize features extracted from\ngeneral pictures by a deep neural network are portable to classify breast\ncancer cell types. The ability to recognize distinct cell types within\ntumors would provide a powerful tool for analyzing clinical samples.","crownCaption":"Poster presented at the University of Wisconsin–Madison Undergraduate Symposium.","crownShowBorder":true,"bibtex":"@inproceedings{wang_using_poster_2018,\n  title = {Using Transfer Learning to Classify Breast Cancer Cells with Fluorescence Imaging},\n  language = {en},\n  conference = {University of Wisconsin–Madison Undergraduate Symposium},\n  author = {Wang, Zijie J. and Heaster, Tiffany M. and Yin, Quan and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  year = {2018}\n}"}],"talk":[{"name":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization","events":[{"time":"Oct. 2020","place":"IEEE Visualization Conference","url":"http://ieeevis.org/year/2020/welcome"},{"time":"Aug. 2020","place":"Deepkapha LiveAI","url":"https://deepkapha.ai/","video":"https://www.youtube.com/watch?v=56bkWcMfN7I"}]},{"name":"Classifying T cell activity with convolutional neural networks","events":[{"time":"Nov. 2019","place":"Out in Science, Technology, Engineering, and Mathematics (oSTEM) Conference","url":"https://www.ostem.org/"},{"time":"April 2019","place":"UW–Madison Senior Honors Thesis Symposium","url":"https://honors.ls.wisc.edu/wp-content/uploads/sites/1038/2019/04/SymposiumSchedule2019email.pdf"}]}],"award":[{"name":"Bosch Research Gift Funding ($37k)","description":"My visual analytics research during my internship results in $37,000 gift funding from Bosch.","url":"https://www.bosch.com/research/","time":"Jan. 2021"},{"name":"Dean's List","description":"Achieved at least a 3.60 GPA as freshmen and sophomores, a 3.85 GPA as\njuniors and seniors.","url":"https://registrar.wisc.edu/deanslist/","time":"Aug. 2015 – May 2019"},{"name":"University Book Store Academic Excellence Award ($1k)","description":"An award recognizing undergraduate students who have completed an\noutstanding independent project, such as a senior thesis.","url":"https://awards.advising.wisc.edu/campus-wide-award-recipients/2016-university-book-store-award-recipients/","time":"May 2019"},{"name":"Honors Senior Thesis Summer Research Grant ($3k)","description":"A research grant funding students to undertake more demanding and extensive\nsenior thesis research projects.","url":"https://honors.ls.wisc.edu/senior-thesis-summer-research-grants/","time":"June 2018"},{"name":"Welton Summer Sophomore Apprenticeship ($2.5k)","description":"A research grant awarded to talented students to participate in actual,\ncutting-edge research.","url":"https://honors.ls.wisc.edu/welton-summer-sophomore-apprenticeships/","time":"June 2017"}],"teaching":[{"title":"Graduate Teaching Assistant","school":"Georgia Institute of Technology","schoolURL":"https://www.gatech.edu/","course":"Data & Visual Analytics (CSE 6242)","courseURL":"https://poloclub.github.io/cse6242-2020fall-online/","instructor":"Duen Horng (Polo) Chau","timeStart":"Aug. 2020","timeEnd":"Dec. 2020","place":"Atlanta, GA","description":"Lead homework designs for data visualizations, hold weekly office hours, and answer student questions on Piazza. The course had 1277 graduate students enrolled.\n"},{"title":"Undergraduate Teaching Assistant","school":"University of Wisconsin–Madison","schoolURL":"https://www.wisc.edu/","course":"Computer Graphics (CS 559)","courseURL":"https://graphics.cs.wisc.edu/WP/cs559-sp2019/overview/","instructor":"Michael Gleicher","timeStart":"Dec. 2018","timeEnd":"May 2019","place":"Madison, WI","description":"Create course notes and weekly assignments, hold weekly office hours, and answer student questions on Piazza. The course had 180 undergraduates enrolled.\n"},{"title":"Notetaker","school":"University of Wisconsin–Madison","schoolURL":"https://www.wisc.edu/","course":"McBurney Disability Resource Center","courseURL":"https://mcburney.wisc.edu/","timeStart":"Sep. 2016","timeEnd":"May 2019","place":"Madison, WI","description":"Provide clearly-written math and statistics notes to students with disability, answer course-related questions.\n"},{"title":"Academic Coach","school":"University of Wisconsin–Madison","schoolURL":"https://www.wisc.edu/","course":"Division of Diversity, Equity and Educational Achievement","courseURL":"https://diversity.wisc.edu/about/about-ddeea/","timeStart":"Nov. 2016","timeEnd":"May 2017","place":"Madison, WI","description":"Mentor undergraduate students in DDEEA programs for Data Structure course, design two worksheets and provided detailed solutions every week.\n"},{"title":"Tutor","school":"University of Wisconsin–Madison","schoolURL":"https://www.wisc.edu/","course":"Greater University Tutoring Service","courseURL":"https://guts.wisc.edu/","timeStart":"Jan. 2016","timeEnd":"Jan. 2017","place":"Madison, WI","description":"Instruct peers one-on-one in programming and math problems for three hours weekly, lead review sections to help students study for calculus exams.\n"}],"mentoring":[{"name":"Jon Saad-Falcon","timeStart":"May 2020","timeEnd":"Present","degree":"B.S. in Computer Science, Georgia Institute of Technology"},{"name":"Robert Turko","timeStart":"Aug. 2019","timeEnd":"Present","degree":"B.S. in Computer Science, Georgia Institute of Technology","description":"Machine learning and visualization","awards":["PURA Travel Award (2020)"]},{"name":"Omar Shaikh","timeStart":"Aug. 2019","timeEnd":"Present","degree":"B.S. in Computer Science, Georgia Institute of Technology","awards":["Outstanding Freshman Award (2020)"]}],"service":{"review":[{"venue":"IEEE Visual Analytics Science and Technology","venueShort":"VAST","url":"http://ieeevis.org","years":[{"year":2020,"yearURL":"http://ieeevis.org/year/2020/welcome"}]},{"venue":"IEEE Pacific Visualization Symposium","venueShort":"PacificVis","url":"https://ieeexplore.ieee.org/xpl/conhome/1001657/all-proceedings","years":[{"year":2021,"yearURL":"http://vis.tju.edu.cn/pvis2021/"}]},{"venue":"EG Conference on Visualization","venueShort":"EuroVis","url":"https://www.eurovis.org/","years":[{"year":2021,"yearURL":"https://conferences.eg.org"}]},{"venue":"ACM Conference on Information and Knowledge Management","venueShort":"CIKM","url":"https://dl.acm.org/conference/cikm","years":[{"year":2020,"yearURL":"https://www.cikm2020.org/"}]},{"venue":"Journal of Open Source Software","venueShort":"JOSS","url":"https://joss.theoj.org/","years":[{"year":2020}]}],"pc":[{"venue":"ACM Conference on Information Retrieval","venueShort":"SIGIR","url":"https://sigir.org/","years":[{"year":2021,"yearURL":"https://sigir.org/sigir2021/index.html"}]}],"membership":[{"org":"Institute of Electrical and Electronics Engineers","orgShort":"IEEE","orgURL":"https://www.ieee.org/","timeStart":"July 2019","timeEnd":"Present"},{"org":"Association for Computing Machinery","orgShort":"ACM","orgURL":"https://www.acm.org/","timeStart":"Dec. 2019","timeEnd":"Present"}]},"reference":[{"name":"Polo Chau","position":"Associate Professor","department":"School of Computational Science and Engineering","departmentURL":"https://cse.gatech.edu/","institution":"Georgia Institute of Technology","institutionURL":"https://www.gatech.edu","url":"https://cc.gatech.edu/~dchau/"},{"name":"Liang Gou","position":"Principal Research Scientist","department":"Human–Machine Interaction","departmentURL":"https://www.bosch.us/our-company/bosch-in-the-usa/sunnyvale/","institution":"Bosch Research","institutionURL":"https://www.bosch.us/our-company/bosch-in-the-usa/sunnyvale/","url":"https://www.linkedin.com/in/lianggou/"},{"name":"Anghony Gitter","position":"Assistant Professor","department":"Department of Biostatistics and Medical Informatics","departmentURL":"https://www.biostat.wisc.edu/","institution":"University of Wisconsin–Madison","institutionURL":"https://www.wisc.edu","url":"https://www.biostat.wisc.edu/~gitter/"},{"name":"Michael Gleicher","position":"Professor","department":"Department of Computer Sciences","departmentURL":"https://www.cs.wisc.edu/","institution":"University of Wisconsin–Madison","institutionURL":"https://www.wisc.edu","url":"http://pages.cs.wisc.edu/~gleicher/"}],"skill":[{"group":"Programming","items":["Python","JavaScript","Swift","R","Julia","PyTorch","TensorFlow","Keras","HTML","CSS","LaTeX","SQL","C++","Git"]},{"group":"Design","items":["Affinity Designer","Affinity Photo","Final Cut Pro","Sketch","Keynote","Illustrator","Photoshop"]},{"group":"HCI","items":["Think-aloud protocol","User Personas","Rapid Paper Prototyping","Affinity Diagraming"]}],"news":[{"date":"Aug. 29, 2020","news":"Two papers,\n<a href='papers/cnn-explainer'>CNN Explainer</a>\nand\n<a href='papers/bluff'>Bluff</a>,\nare accepted for IEEE VIS 2020!"},{"date":"June 18, 2020","news":"Started my first internship at <a href='https://www.bosch.us/our-company/bosch-in-the-usa/sunnyvale/' target='_self'>Bosch (Sunnyvale)</a> , working with <a href='https://www.linkedin.com/in/lianggou/' target='_self'>Liang Gou</a> on visual analytics for autonomous driving.\n"},{"date":"April 30, 2020","news":"Posted <a href='https://arxiv.org/abs/2004.15004' target='_self'>CNN Explainer paper</a> on arXiv. CNN Explainer an <a href='http://poloclub.github.io/cnn-explainer/' target='_self'>interactive tool</a> that helps beginners learn CNNs. It is also <a href='https://github.com/poloclub/cnn-explainer' target='_self'>open-sourced</a> on GitHub.\n"},{"date":"Jan 15, 2020","news":"Two papers, <a href='papers/cnn-101' target='_self'>CNN 101</a> and <a href='papers/massif' target='_self'>Massif</a>, are accepted for CHI 2020 Late-Breaking Works!\n"},{"date":"Nov. 15, 2019","news":"I will present my T Cell Classification <a href='/pdf/glbio_2019.pdf' target='_self'>poster</a> at <a href='https://ostem.org/page/conference-2019' target='_self'>oSTEM'19</a>. See you in Detroit (really miss the Midwest winter!). \n"},{"date":"Oct. 1, 2019","news":"I will attend <a href='http://ieeevis.org/year/2019/welcome' target='_self'>VIS'19</a> in Vancouver. Come to talk to me :) \n"},{"date":"Aug. 15, 2019","news":"Submitted my first <a href='https://www.biorxiv.org/content/10.1101/737346v1' target='_self'>paper</a> on bioRxiv. Check it out!\n"},{"date":"May. 11, 2019","news":"<a href='https://web.archive.org/web/20200928184759/https://commencement.wisc.edu/content/uploads/2019/05/Saturday-May-11th-2019-Spring-Commencement-Ceremony-1.pdf' target='_self'>I graduated!</a> Will always be a proud badger!! 🦡\n"},{"date":"April 20, 2019","news":"Honored to receive the 2019 <a href='https://awards.advising.wisc.edu/campus-wide-award-recipients/2016-university-book-store-award-recipients/' target='_self'> University Book Store Academic Excellence Award</a>.\n"},{"date":"April 10, 2019","news":"Excited to present my T-cell classification project as a poster in     <a href='https://www.iscb.org/glbio2019' target='_self'>GLBIO'19</a>.\n"}],"featured":[{"id":"cnn-explainer","featureImg":"/images/teasers/cnn-explainer.png"},{"id":"bluff","featureImg":"/images/teasers/bluff.png"},{"id":"unmask","featureImg":"/images/teasers/unmask.png"},{"id":"ai-guideline","featureImg":"/images/teasers/ai-guideline.png"}],"project":[{"name":"Clip2imgur","repo":"xiaohk/clip2imgur","teaser":"/images/teasers/project-clip2imgur.png","award":{"name":"Featured on Imgur","url":"https://help.imgur.com/hc/en-us/articles/209592766-Tools-for-Imgur"},"description":"Convenient macOS command line tool for uploading screen-shots from the clipboard to Imgur. \n"},{"name":"FaceData","repo":"xiaohk/FaceData","showStar":true,"teaser":"/images/teasers/project-face.jpg","description":"MacOS GUI to auto-annotate facial landmarks from a video. Landmarks can be used to train GANs.\n"},{"name":"Graphics on the Web","repo":"xiaohk/CS559-computational-graphics","demo":"http://jayw-www.cs.wisc.edu/cs559/p10/","teaser":"/images/teasers/project-graphics.png","description":"Interactive 2D, 2.5D and 3D computational graphics with shaders and textures, created with HTML canvas and webGL.\n"},{"name":"Group Assignment Problem","repo":"xiaohk/CS559-computational-graphics","demo":"https://nbviewer.jupyter.org/github/xiaohk/CS524-Group-Assignment-Optimization/blob/master/Wang.ipynb","teaser":"/images/teasers/project-optim.png","award":{"name":"Best project","url":"https://web.archive.org/web/20200917025916/https://laurentlessard.com/teaching/524-intro-to-optimization/"},"description":"Flexible and robust Mixed Integer Quadratic Programming model written in Julia to solve a real-life optimization problem.\n"},{"name":"Dean's list Vis","demo":"http://jayw-www.cs.wisc.edu/d3-china-map/","repo":"xiaohk/d3-china-map","teaser":"/images/teasers/project-map.png","description":"Interactive geo-visualization to explore where UW–Madison Chinese students are from.\n"},{"name":"Yelp Sentiment","repo":"xiaohk/stat333_project_2","teaser":"/images/teasers/project-review.png","award":{"name":"Kaggle winner","url":"https://www.kaggle.com/c/uw-madison-sp17-stat333"},"description":"Predicting Yelp ratings based on text comments of restaurants at Madison Wisconsin.\n"}]}