<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest> <link href=favicon.ico rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Roboto+Mono:ital,wght@0,100;0,300;0,400;0,500;0,700;1,100;1,300;1,400;1,500;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet> <link href=client/main.2434616356.css rel=stylesheet><link href=client/index.9cf32b69.css rel=stylesheet><link href=client/client.eff93043.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Jay Wang</title><meta content="Jay Wang" class=svelte-70v5wq name=title><meta content="Zijie Jay Wang is a Ph.D. student in the College of Computing at Georgia Tech researching the intersection of machine learning and data visualization." class=svelte-70v5wq name=description><meta content=website class=svelte-70v5wq property=og:type><meta content=https://zijie.wang/ class=svelte-70v5wq property=og:url><meta content="Jay Wang" class=svelte-70v5wq property=og:title><meta content="Zijie Jay Wang is a Ph.D. student in the College of Computing at Georgia Tech researching the intersection of machine learning and data visualization." class=svelte-70v5wq property=og:description><meta content=https://zijie.wang/images/teasers/preview.png class=svelte-70v5wq property=og:image><meta content=summary_large_image class=svelte-70v5wq property=twitter:card><meta content=https://zijie.wang/ class=svelte-70v5wq property=twitter:url><meta content="Jay Wang" class=svelte-70v5wq property=twitter:title><meta content="Zijie Jay Wang is a Ph.D. student in the College of Computing at Georgia Tech researching the intersection of machine learning and data visualization." class=svelte-70v5wq property=twitter:description><meta content=https://zijie.wang/images/teasers/preview.png class=svelte-70v5wq property=twitter:image><meta content=@jay4w class=svelte-70v5wq property=twitter:site><meta content=@jay4w class=svelte-70v5wq property=twitter:creator><script class=svelte-70v5wq async src="https://www.googletagmanager.com/gtag/js?id=UA-130177683-1"></script><script class=svelte-70v5wq>window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-130177683-1'); </script><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <main class=svelte-1jybnva> <div class="svelte-70v5wq page"><div class="svelte-70v5wq left-padding"></div> <div class="svelte-70v5wq content"> <div class="svelte-70v5wq block-tb"><h1 class="svelte-70v5wq header name">Zijie Jay Wang</h1> <div class="svelte-70v5wq block-lr"><div class="svelte-70v5wq name-text"><p class=svelte-70v5wq>Hi, I'm Jay. I'm a Machine Learning Ph.D. student in the <a href=https://www.cc.gatech.edu/ target=_blank>College of Computing</a> at <a href=https://gatech.edu/ target=_blank>Georgia Tech</a> adviced by <a href=https://www.cc.gatech.edu/~dchau target=_blank>Polo Chau</a>. </p> <p class=svelte-70v5wq>My research focuses on making AI more accessible, interpretable, and accountable, by designing and developing novel <span class="svelte-70v5wq strong">interactive interfaces</span> for people to easily and enjoyably interact with <span class="svelte-70v5wq strong">machine learning systems</span> at scale. </p> <p class=svelte-70v5wq>I recieved my B.S. from <a href=https://wisc.edu target=_blank>UW–Madison</a>, where I worked closely with <a href=https://www.biostat.wisc.edu/~gitter target=_blank>Anthony Gitter</a>, <a href=http://pages.cs.wisc.edu/~gleicher/ target=_blank>Michael Gleicher</a>, and <a href=http://homepages.cae.wisc.edu/~hu/ target=_blank>Yu Hen Hu</a>. </div> <div class="svelte-70v5wq name-pic"><img alt="Jay Wang" class=svelte-70v5wq src=images/jay.jpg> <div class="svelte-70v5wq block-tb" style="padding:0 5px"><div class="svelte-70v5wq icons"><a href=https://github.com/xiaohk target=_self class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></a> <a href="https://scholar.google.com/citations?user=eouAYvcAAAAJ&hl=en" target=_self class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#graduation-cap-solid></use></svg></a> <a href=https://orcid.org/0000-0003-4360-1423 target=_self class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#orcid-brands></use></svg></a> <a href=https://twitter.com/Jay4w target=_self class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#twitter-brands></use></svg></a> <a href=https://www.linkedin.com/in/zijiewang/ target=_self class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#linkedin-brands></use></svg></a></div> <div class="svelte-70v5wq infos"><div class="svelte-70v5wq label-text" style="font-family:Roboto Mono"><a href=mailto:jayw@gatech.edu target=_self class=svelte-70v5wq>jayw@gatech.edu</a></div> <div class="svelte-70v5wq label-cv"><a href=/cv target=_self class=svelte-70v5wq rel=prefetch>CV</a></div></div></div></div></div> <div class="svelte-70v5wq has-block-name news-block"><div class="svelte-70v5wq block-name">News</div> <div class="svelte-70v5wq news-content"><div class="svelte-70v5wq news"><div class="svelte-70v5wq news-date">Aug. 14, 2020</div> <div class="svelte-70v5wq news-text">My paper <a href=papers/cnn-explainer target=_self>CNN Explainer</a> is accepted for IEEE VIS 2020 VAST (TVCG Journal Track)! </div> </div><div class="svelte-70v5wq news"><div class="svelte-70v5wq news-date">June 18, 2020</div> <div class="svelte-70v5wq news-text">Started my first internship at <a href=https://www.bosch.us/our-company/bosch-in-the-usa/sunnyvale/ target=_self>Bosch (Sunnyvale)</a> , working with <a href=https://www.linkedin.com/in/lianggou/ target=_self>Liang Gou</a> on visual analytics for autonomous driving. </div> </div><div class="svelte-70v5wq news"><div class="svelte-70v5wq news-date">April 30, 2020</div> <div class="svelte-70v5wq news-text">Posted my <a href=https://arxiv.org/abs/2004.15004 target=_self>CNN Explainer paper</a> on arXiv. CNN Explainer an <a href=http://poloclub.github.io/cnn-explainer/ target=_self>interactive tool</a> that helps beginners learn CNNs. It is also <a href=https://github.com/poloclub/cnn-explainer target=_self>open-sourced</a> on GitHub. </div> </div><div class="svelte-70v5wq news"><div class="svelte-70v5wq news-date">Jan 15, 2020</div> <div class="svelte-70v5wq news-text">Two papers, <a href=papers/cnn-101 target=_self>CNN 101</a> and <a href=papers/massif target=_self>Massif</a>, are accepted for CHI 2020 Late-Breaking Works! </div> </div></div> <div class="svelte-70v5wq svg-icon right-margin add-more-button"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#plus-circle-solid></use></svg> show more</div></div> <div class="svelte-70v5wq pub-block has-block-name"><div class="svelte-70v5wq block-name">Featured Publications</div> <div class="svelte-70v5wq news-content"><div class="svelte-70v5wq pub"><a href=/papers/cnn-explainer target=_self><img alt=Thumbnail class=svelte-70v5wq src=/images/teasers/cnn-explainer.png></a> <div class="svelte-70v5wq pub-content"><div class="svelte-70v5wq pub-title"><a href=/papers/cnn-explainer target=_self>CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization </a></div> <div class="svelte-70v5wq pub-author"><a href=https://zijie.wang target=_blank style=font-weight:700>Zijie J. Wang</a>, <a href=https://www.linkedin.com/in/robert-turko/ target=_blank>Robert Turko</a>, <a href=http://oshaikh.com/ target=_blank>Omar Shaikh</a>, <a href=https://haekyu.com target=_blank>Haekyu Park</a>, <a href=https://nilakshdas.com target=_blank>Nilaksh Das</a>, <a href=https://fredhohman.com target=_blank>Fred Hohman</a>, <a href=https://minsuk.com target=_blank>Minsuk Kahng</a>, <a href=https://www.cc.gatech.edu/~dchau target=_blank>Duen Horng (Polo) Chau</a></div> <div class="svelte-70v5wq pub-venue"><a href=https://www.computer.org/csdl/journal/tg target=_self>IEEE Transactions on Visualization and Computer Graphics (TVCG), 2021 </a></div> <div class="svelte-70v5wq pub-icons"><a href=/papers/cnn-explainer target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#link-solid></use></svg></div> <span class=svelte-70v5wq>Project</span> </a> <a href=https://poloclub.github.io/cnn-explainer/ target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#play-solid></use></svg></div> <span class=svelte-70v5wq>Demo</span> </a> <a href=https://youtu.be/HnWIHWFbuUQ target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#film-solid></use></svg></div> <span class=svelte-70v5wq>Video</span> </a> <a href=https://arxiv.org/abs/2004.15004 target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-pdf-solid></use></svg></div> <span class=svelte-70v5wq>PDF</span> </a> <div class="svelte-70v5wq icon-container"><a href=https://github.com/poloclub/cnn-explainer target=_self class="svelte-70v5wq icon-container no-right-margin"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span></a> <div class="svelte-70v5wq star-container"><div style=display:flex>(<a href=https://github.com/poloclub/cnn-explainer/stargazers target=_self class="svelte-70v5wq svg-icon" style=font-weight:500;margin-right:-3px> loading </a>)</div> </div> </div> <div class="svelte-70v5wq icon-container bibtex-button"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#quote-right-solid></use></svg></div> <span class=svelte-70v5wq>BibTeX</span> </div> <div class="svelte-70v5wq icon-container award"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#award-solid></use></svg></div> <div class="svelte-70v5wq award-highlight"><a href=https://web.archive.org/web/20200505121955/https://github.com/trending target=_blank>Trending (#2) on GitHub for 3 days</a></div> </div></div> </div></div> <div class="svelte-70v5wq bibtex hidden"><pre class=svelte-70v5wq>@article{wangCNNExplainerLearning2021,
  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},
  shorttitle = {{{CNN Explainer}}},
  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},
  journal={IEEE Transactions on Visualization and Computer Graphics (TVCG)},
  publisher={IEEE},
  year={2021},
}</pre> </div></div> </div><div class="svelte-70v5wq pub-block"> <div class="svelte-70v5wq news-content"><div class="svelte-70v5wq pub"><a href=/papers/bluff target=_self><img alt=Thumbnail class=svelte-70v5wq src=/images/teasers/bluff.png></a> <div class="svelte-70v5wq pub-content"><div class="svelte-70v5wq pub-title"><a href=/papers/bluff target=_self>Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks </a></div> <div class="svelte-70v5wq pub-author"><a href=https://nilakshdas.com target=_blank>Nilaksh Das</a>*, <a href=https://haekyu.com target=_blank>Haekyu Park</a>*, <a href=https://zijie.wang target=_blank style=font-weight:700>Zijie J. Wang</a>, <a href=https://fredhohman.com target=_blank>Fred Hohman</a>, <a href=https://www.robfirstman.com/ target=_blank>Robert Firstman</a>, <a href=https://www.linkedin.com/in/emily-rogers-1a828598 target=_blank>Emily Rogers</a>, <a href=https://www.cc.gatech.edu/~dchau target=_blank>Duen Horng (Polo) Chau</a></div> <div class="svelte-70v5wq icon-container comment">(*Authors contributed equally)</div> <div class="svelte-70v5wq pub-venue"><a href=http://ieeevis.org/year/2020/welcome target=_self>IEEE Visualization Conference (VIS), 2020 </a></div> <div class="svelte-70v5wq pub-icons"><a href=/papers/bluff target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#link-solid></use></svg></div> <span class=svelte-70v5wq>Project</span> </a> <a href=https://poloclub.github.io/bluff/ target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#play-solid></use></svg></div> <span class=svelte-70v5wq>Demo</span> </a> <a href=https://arxiv.org/abs/2009.02608 target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-pdf-solid></use></svg></div> <span class=svelte-70v5wq>PDF</span> </a> <div class="svelte-70v5wq icon-container"><a href=https://github.com/poloclub/bluff target=_self class="svelte-70v5wq icon-container no-right-margin"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span></a> </div> <div class="svelte-70v5wq icon-container bibtex-button"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#quote-right-solid></use></svg></div> <span class=svelte-70v5wq>BibTeX</span> </div> </div> </div></div> <div class="svelte-70v5wq bibtex hidden"><pre class=svelte-70v5wq>@article{dasBluffInteractivelyDeciphering2020,
  title={Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks},
  author={Das, Nilaksh and Park, Haekyu and Wang, Zijie J and Hohman, Fred and Firstman, Robert and Rogers, Emily and Chau, Duen Horng},
  booktitle={IEEE Visualization Conference (VIS)},
  publisher={IEEE},
  year={2020}
}</pre> </div></div> </div><div class="svelte-70v5wq pub-block pub-block-last"> <div class="svelte-70v5wq news-content"><div class="svelte-70v5wq pub"><a href=/papers/unmask target=_self><img alt=Thumbnail class=svelte-70v5wq src=/images/teasers/unmask.png></a> <div class="svelte-70v5wq pub-content"><div class="svelte-70v5wq pub-title"><a href=/papers/unmask target=_self>UnMask: Adversarial Detection and Defense Through Robust Feature Alignment </a></div> <div class="svelte-70v5wq pub-author"><a href=https://www.scottfreitas.com/ target=_blank>Scott Freitas</a>, <a href=https://www.cc.gatech.edu/~schen351/ target=_blank>Shang-Tse Chen</a>, <a href=https://zijie.wang target=_blank style=font-weight:700>Zijie J. Wang</a>, <a href=https://www.cc.gatech.edu/~dchau target=_blank>Duen Horng (Polo) Chau</a></div> <div class="svelte-70v5wq pub-venue"><a href=https://arxiv.org/abs/2002.09576 target=_self>arXiv:22002.09576 [cs] , 2020 </a></div> <div class="svelte-70v5wq pub-icons"><a href=/papers/unmask target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#link-solid></use></svg></div> <span class=svelte-70v5wq>Project</span> </a> <a href=https://arxiv.org/abs/2002.09576 target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-pdf-solid></use></svg></div> <span class=svelte-70v5wq>PDF</span> </a> <div class="svelte-70v5wq icon-container"><a href=https://github.com/unmaskd/unmask target=_self class="svelte-70v5wq icon-container no-right-margin"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span></a> </div> <div class="svelte-70v5wq icon-container bibtex-button"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#quote-right-solid></use></svg></div> <span class=svelte-70v5wq>BibTeX</span> </div> </div> </div></div> <div class="svelte-70v5wq bibtex hidden"><pre class=svelte-70v5wq>@article{freitasUnMaskAdversarialDetection2020,
  title = {{{UnMask}}: {{Adversarial Detection}} and {{Defense Through Robust Feature Alignment}}},
  shorttitle = {{{UnMask}}},
  author = {Freitas, Scott and Chen, Shang-Tse and Wang, Zijie J. and Chau, Duen Horng},
  year = {2020},
  archivePrefix = {arXiv},
  eprint = {2002.09576},
  eprinttype = {arxiv},
  journal = {arXiv:2002.09576}
}</pre> </div></div> <a href=/cv#publications target=_self style=padding:0><div class="svelte-70v5wq svg-icon right-margin show-all-button"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#external-link-alt-solid></use></svg> see all publications </div> </a> </div> <div class="svelte-70v5wq project"><div class="svelte-70v5wq block-name">Fun Projects</div> <div class="svelte-70v5wq project-card"><a href=https://github.com/xiaohk/clip2imgur><img alt=project-teaser class=svelte-70v5wq src=/images/teasers/project-clip2imgur.png></a> <div class="svelte-70v5wq project-description"><div class="svelte-70v5wq project-text"><span class="svelte-70v5wq project-name">Clip2imgur:</span> <span class="svelte-70v5wq project-detail">Convenient macOS command line tool for uploading screen-shots from the clipboard to Imgur. </span></div> <div class="svelte-70v5wq pub-icons"><a href=https://github.com/xiaohk/clip2imgur target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span> </a> <div class="svelte-70v5wq icon-container award"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#award-solid></use></svg></div> <div class="svelte-70v5wq award-highlight"><a href=https://help.imgur.com/hc/en-us/articles/209592766-Tools-for-Imgur target=_blank>Featured on Imgur</a></div> </div> </div></div> </div><div class="svelte-70v5wq project-card"><a href=https://github.com/xiaohk/FaceData><img alt=project-teaser class=svelte-70v5wq src=/images/teasers/project-face.jpg></a> <div class="svelte-70v5wq project-description"><div class="svelte-70v5wq project-text"><span class="svelte-70v5wq project-name">FaceData:</span> <span class="svelte-70v5wq project-detail">MacOS GUI to auto-annotate facial landmarks from a video. Landmarks can be used to train GANs. </span></div> <div class="svelte-70v5wq pub-icons"><a href=https://github.com/xiaohk/FaceData target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span> </a> </div></div> </div><div class="svelte-70v5wq project-card"><a href=https://github.com/xiaohk/CS559-computational-graphics><img alt=project-teaser class=svelte-70v5wq src=/images/teasers/project-graphics.png></a> <div class="svelte-70v5wq project-description"><div class="svelte-70v5wq project-text"><span class="svelte-70v5wq project-name">Graphics on the Web:</span> <span class="svelte-70v5wq project-detail">Interactive 2D, 2.5D and 3D computational graphics with shaders and textures, created with HTML canvas and webGL. </span></div> <div class="svelte-70v5wq pub-icons"><a href=https://github.com/xiaohk/CS559-computational-graphics target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span> </a> <a href=http://jayw-www.cs.wisc.edu/cs559/p10/ target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#play-solid></use></svg></div> <span class=svelte-70v5wq>Demo</span> </a> </div></div> </div><div class="svelte-70v5wq project-card"><a href=https://github.com/xiaohk/CS559-computational-graphics><img alt=project-teaser class=svelte-70v5wq src=/images/teasers/project-optim.png></a> <div class="svelte-70v5wq project-description"><div class="svelte-70v5wq project-text"><span class="svelte-70v5wq project-name">Group Assignment Problem:</span> <span class="svelte-70v5wq project-detail">Flexible and robust Mixed Integer Quadratic Programming model written in Julia to solve a real-life optimization problem. </span></div> <div class="svelte-70v5wq pub-icons"><a href=https://github.com/xiaohk/CS559-computational-graphics target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span> </a> <a href=https://nbviewer.jupyter.org/github/xiaohk/CS524-Group-Assignment-Optimization/blob/master/Wang.ipynb target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#play-solid></use></svg></div> <span class=svelte-70v5wq>Demo</span> </a> <div class="svelte-70v5wq icon-container award"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#award-solid></use></svg></div> <div class="svelte-70v5wq award-highlight"><a href=https://laurentlessard.com/teaching/524-intro-to-optimization/ target=_blank>Best project</a></div> </div> </div></div> </div><div class="svelte-70v5wq project-card"><a href=https://github.com/xiaohk/d3-china-map><img alt=project-teaser class=svelte-70v5wq src=/images/teasers/project-map.png></a> <div class="svelte-70v5wq project-description"><div class="svelte-70v5wq project-text"><span class="svelte-70v5wq project-name">Dean's list Vis:</span> <span class="svelte-70v5wq project-detail">Interactive geo-visualization to explore where UW–Madison Chinese students are from. </span></div> <div class="svelte-70v5wq pub-icons"><a href=https://github.com/xiaohk/d3-china-map target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span> </a> <a href=http://jayw-www.cs.wisc.edu/d3-china-map/ target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#play-solid></use></svg></div> <span class=svelte-70v5wq>Demo</span> </a> </div></div> </div><div class="svelte-70v5wq project-card"><a href=https://github.com/xiaohk/stat333_project_2><img alt=project-teaser class=svelte-70v5wq src=/images/teasers/project-review.png></a> <div class="svelte-70v5wq project-description"><div class="svelte-70v5wq project-text"><span class="svelte-70v5wq project-name">Yelp Sentiment:</span> <span class="svelte-70v5wq project-detail">Predicting Yelp ratings based on text comments of Madison restaurants. </span></div> <div class="svelte-70v5wq pub-icons"><a href=https://github.com/xiaohk/stat333_project_2 target=_self class="svelte-70v5wq icon-container"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-70v5wq>Code</span> </a> <div class="svelte-70v5wq icon-container award"><div class="svelte-70v5wq svg-icon"><svg class=svelte-70v5wq viewBox="0 0 100 100"><use xlink:href=/sprite.svg#award-solid></use></svg></div> <div class="svelte-70v5wq award-highlight"><a href=https://www.kaggle.com/c/uw-madison-sp17-stat333 target=_blank>In-class Kaggle winner</a></div> </div> </div></div> </div> </div></div></div> <div class="svelte-70v5wq right-padding"></div></div></main> <footer class=svelte-1pltrfa><div class=left-padding></div> <div class="svelte-1pltrfa footer-main"><div class="svelte-1pltrfa footer"><div class="svelte-1pltrfa footer-item"><span>Designed and built by <a href=. class="svelte-1pltrfa raleway">Jay Wang</a> with</span> <div class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#t-heart></use></svg></div> <span>using <a href=https://svelte.dev/ target=_self class=svelte-1pltrfa>Svelte</a> and <a href=https://sapper.svelte.dev/ target=_self class=svelte-1pltrfa>Sapper</a>.</span></div> <div class="svelte-1pltrfa footer-other"><div class=footer-icons><a href=cv target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-alt-regular></use></svg></a> <a href=https://github.com/xiaohk target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></a> <a href=http://twitter.com/jay4w target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#twitter-brands></use></svg></a> <a href="https://scholar.google.com/citations?user=eouAYvcAAAAJ&hl=en" target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#graduation-cap-solid></use></svg></a> <a href=https://www.linkedin.com/in/zijiewang/ target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#linkedin-brands></use></svg></a> <a href=mailto:jayw@gatech.edu target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#envelope-regular></use></svg></a></div> <div>© 2020 <a href=. class="svelte-1pltrfa raleway">Zijie Jay Wang</a></div></div></div></div> <div class=right-padding></div></footer></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,(function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa){return {data:{people:{"Zijie J. Wang":{url:"https:\u002F\u002Fzijie.wang",isMe:f},"Anthony Gitter":{url:"https:\u002F\u002Fwww.biostat.wisc.edu\u002F~gitter"},"Melissa C. Skala":{url:"https:\u002F\u002Fmorgridge.org\u002Fresearch\u002Fmedical-engineering\u002Foptical-microscopy"},"Alex J. Walsh":{url:"https:\u002F\u002Fqoil.engr.tamu.edu"},"Duen Horng (Polo) Chau":{url:y},"Polo Chau":{url:y},"Fred Hohman":{url:"https:\u002F\u002Ffredhohman.com"},"Minsuk Kahng":{url:"https:\u002F\u002Fminsuk.com"},"Haekyu Park":{url:"https:\u002F\u002Fhaekyu.com"},"Nilaksh Das":{url:"https:\u002F\u002Fnilakshdas.com"},"Robert Turko":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Frobert-turko\u002F"},"Omar Shaikh":{url:"http:\u002F\u002Foshaikh.com\u002F"},"Michael Gleicher":{url:z},"Yu Hen Hu":{url:"http:\u002F\u002Fhomepages.cae.wisc.edu\u002F~hu\u002F"},"Tiffany M. Heaster":{url:"https:\u002F\u002Fmorgridge.org\u002Fprofile\u002Ftiffany-heaster\u002F"},"Quan Yin":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fquan-yin\u002F"},"Emily Rogers":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Femily-rogers-1a828598"},"Robert Firstman":{url:"https:\u002F\u002Fwww.robfirstman.com\u002F"},"Scott Freitas":{url:"https:\u002F\u002Fwww.scottfreitas.com\u002F"},"Shang-Tse Chen":{url:"https:\u002F\u002Fwww.cc.gatech.edu\u002F~schen351\u002F"},"Jon Saad-Falcon":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fjonsaadfalcon\u002F"},"Austin P. Wright":{url:"https:\u002F\u002Faustinpwright.com\u002F"},"Sasha Richardson":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fsasha-richardson\u002F"},"Siwei Li":{url:"https:\u002F\u002Frsli.github.io\u002F"},"Zhiyan Zhou":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Ffrank-zhou-b19515159\u002F"},"Anish Upadhayay":{url:"https:\u002F\u002Fgithub.com\u002Faupadhayay3"},"Susanta Routray":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fsusantaroutray\u002F"},"Matthew Hull":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fmdhull\u002F"}},education:[{school:t,schoolURL:"https:\u002F\u002Fwww.gatech.edu",place:A,timeStart:k,timeEnd:g,descriptions:["Ph.D. in Machine Learning"],advisors:[b]},{school:c,schoolURL:i,place:d,timeStart:"Sept. 2015",timeEnd:o,descriptions:["Bachelor of Science (B.S.), GPA: 3.95\u002F4.00","Majors: Computer Sciences (Honor), Statistics (Honor), Mathematics"],advisors:[l,p,B],thesis:{title:"Classifying T Cell Activity with Convolutional Neural Networks",file:"pdf\u002Fundergrad-thesis.pdf"}}],experience:[{institution:t,place:A,position:"Ph.D. Researcher",group:C,timeStart:k,timeEnd:g,mentors:[b],description:"Member of the Polo Club of Data Science where we innovate scalable, interactive, and interpretable tools that amplify human's ability to understand and interact with billion-scale data and machine learning models.\n",institutionURL:i,groupURL:D,type:q},{institution:c,place:d,position:E,group:F,timeStart:G,timeEnd:"June 2019",mentors:[p],description:"Design and implement a visual analytics tool for recommender system resaerchers. Interactively visualized user-item rating matrix with statistics-conditioned sub-sampling to spot abnormal ratings and predictions.\n",institutionURL:i,groupURL:D,type:q},{institution:"Morgridge Institute for Research",place:d,position:E,group:"John W. and Jeanne M. Rowe Center for Research in Virology",timeStart:H,timeEnd:k,mentors:[l],description:"Classify T-cell and breast cancer cell types using fluorescent images with machine learning classifiers with a gradient of complexity. Interpre feature representations of each classifiers. Analyze about 1 million 5-channel cell-painting images of bone tumor cells. Explore latent space between image space and chemical molecule space.\n",institutionURL:"https:\u002F\u002Fmorgridge.org",groupURL:"https:\u002F\u002Fmorgridge.org\u002Fresearch\u002Fvirology\u002F",type:q},{institution:c,place:d,position:"Research Assistant",group:"Electrical & Computer Engineering",timeStart:"Feb. 2017",timeEnd:H,mentors:[B],description:"Study how to track car driver’s head position and orientation from low-qualitytraffic video. Develop semi-automatic video annotation software with Viola-Jones frontal facedetector for training object tracking algorithms. Implement real-time face tracking algorithms on iOS devices. Train a facial reenactment model using GANs and port it to iOS device.\n",institutionURL:i,groupURL:"https:\u002F\u002Fwww.engr.wisc.edu\u002Fdepartment\u002Felectrical-computer-engineering\u002F",type:q}],publication:[{id:I,title:"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization",authors:[a,u,m,h,j,r,J,b],venue:"IEEE Transactions on Visualization and Computer Graphics",venueURL:"https:\u002F\u002Fwww.computer.org\u002Fcsdl\u002Fjournal\u002Ftg",venueShort:"TVCG",year:2021,url:"\u002Fpapers\u002Fcnn-explainer",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2004.15004",repo:"poloclub\u002Fcnn-explainer",showStar:f,awards:[{name:"Trending (#2) on GitHub for 3 days",url:"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20200505121955\u002Fhttps:\u002F\u002Fgithub.com\u002Ftrending"}],video:"https:\u002F\u002Fyoutu.be\u002FHnWIHWFbuUQ",demo:"https:\u002F\u002Fpoloclub.github.io\u002Fcnn-explainer\u002F",abstract:"Deep learning's great success motivates many practitioners and students to\nlearn about this exciting technology. However, it is often challenging for\nbeginners to take their first step due to the complexity of understanding\nand applying deep learning. We present CNN Explainer, an interactive\nvisualization tool designed for non-experts to learn and examine\nconvolutional neural networks (CNNs), a foundational deep learning model\narchitecture. Our tool addresses key challenges that novices face while\nlearning about CNNs, which we identify from interviews with instructors and\na survey with past students. CNN Explainer tightly integrates a model\noverview that summarizes a CNN's structure, and on-demand, dynamic visual\nexplanation views that help users understand the underlying components of\nCNNs. Through smooth transitions across levels of abstraction, our tool\nenables users to inspect the interplay between low-level mathematical\noperations and high-level model structures. A qualitative user study shows\nthat CNN Explainer helps users more easily understand the inner workings of\nCNNs, and is engaging and enjoyable to use. We also derive design lessons\nfrom our study. Developed using modern web technologies, CNN Explainer runs\nlocally in users' web browsers without the need for installation or\nspecialized hardware, broadening the public's education access to modern\ndeep learning techniques.",crownCaption:"With CNN Explainer, learners can visually examine how Convolutional Neural\nNetworks (CNNs) transform input images into classification predictions\n(e.g., predicting espresso for an image of a coffee cup), and interactively\nlearn about their underlying mathematical operations. In this example, a\nlearner uses CNN Explainer to understand how convolutional layers work\nthrough three tightly integrated views, each explaining the convolutional\nprocess in increasing levels of detail. (A) The Overview visualizes a CNN\narchitecture where each neuron is encoded as a square with a heatmap\nrepresenting the neuron’s output, and each edge connects the neuron with its\ncorresponding inputs and outputs. (B) Clicking a neuron reveals how its\nactivations are computed by the previous layer’s neurons, displaying the\noften-overlooked intermediate computation through animations of sliding\nkernels. (C) The Convolutional Interactive Formula View allows users to\ninteractively inspect the underlying mathematics of the dot-product\noperation core to convolution, through hovering the 3×3 kernel over the\ninput, and interactively studying the corresponding output. For clarity,\nvisibility of Overview and annotation text is improved, and the overlay is\nre-positioned.",bibtex:"@article{wangCNNExplainerLearning2021,\n  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},\n  shorttitle = {{{CNN Explainer}}},\n  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  journal={IEEE Transactions on Visualization and Computer Graphics (TVCG)},\n  publisher={IEEE},\n  year={2021},\n}"},{id:K,title:"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks",authors:[j,h,a,r,L,M,b],equals:[j,h],venue:"IEEE Visualization Conference",venueURL:N,venueShort:"VIS",year:e,url:"\u002Fpapers\u002Fbluff",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2009.02608",repo:"poloclub\u002Fbluff",showStar:false,demo:"https:\u002F\u002Fpoloclub.github.io\u002Fbluff\u002F",abstract:"Deep neural networks (DNNs) are now commonly used in many domains. However,\nthey are vulnerable to adversarial attacks: carefully crafted perturbations\non data inputs that can fool a model into making incorrect predictions.\nDespite significant research on developing DNN attack and defense\ntechniques, people still lack an understanding of how such attacks penetrate\na model's internals. We present Bluff, an interactive system for\nvisualizing, characterizing, and deciphering adversarial attacks on\nvision-based neural networks. Bluff allows people to flexibly visualize and\ncompare the activation pathways for benign and attacked images, revealing\nmechanisms that adversarial attacks employ to inflict harm on a model. Bluff\nis open-sourced and runs in modern web browsers. ",crownCaption:"With Bluff, users interactively visualize how adversarial attacks penetrate\na deep neural network to induce incorrect outcomes. Here, a user inspects\nwhy Inception V1 misclassifies adversarial giant panda images, crafted by\nthe Projected Gradient Descent (PGD) attack, as armadillo. PGD successfully\nperturbed pixels to induce the “brown bird” feature, an appearance more\nlikely shared by an armadillo (small, roundish, brown body) than a panda,\nactivating more features that contribute to the armadillo (mis)classification\n(e.g., “scales,” “bumps,” “mesh”). The adversarial pathways, formed by these\nneurons and their connections, overwhelm the benign panda pathways and lead\nto the ultimate misclassification. (A) Control Side bar allows users to\nspecify what data is to be included and highlighted. (B) Graph Summary View\nvisualizes pathways most activated or changed by an attack as a network\ngraph of neurons (each labeled by the channel ID in its layer) and their\nconnections. When hovering over a neuron, (C) Detail View displays its\nfeature visualization, representative dataset examples, and activation\npatterns over attack strengths.",bibtex:"@article{dasBluffInteractivelyDeciphering2020,\n  title={Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks},\n  author={Das, Nilaksh and Park, Haekyu and Wang, Zijie J and Hohman, Fred and Firstman, Robert and Rogers, Emily and Chau, Duen Horng},\n  booktitle={IEEE Visualization Conference (VIS)},\n  publisher={IEEE},\n  year={2020}\n}"},{id:"argo-lite",title:"Argo Lite: Open-Source Interactive Graph Exploration and Visualization in Browsers",authors:["Siwei Li","Zhiyan Zhou","Anish Upadhayay",m,O,h,a,"Susanta Routray","Matthew Hull",b],venue:"The Conference on Information and Knowledge Management",venueShort:P,venueURL:Q,year:e,url:"\u002Fpapers\u002Fargo-lite",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2008.11844",repo:"poloclub\u002Fargo-graph-lite",showStar:f,type:n,demo:"https:\u002F\u002Fpoloclub.github.io\u002Fargo-graph-lite\u002F",abstract:"Graph data have become increasingly common. Visualizing them helps people\nbetter understand relations among entities. Unfortunately, existing graph\nvisualization tools are primarily designed for single-person desktop use,\noffering limited support for interactive web-based exploration and online\ncollaborative analysis. To address these issues, we have developed Argo\nLite, a new in-browser interactive graph exploration and visualization tool.\nArgo Lite enables users to publish and share interactive graph\nvisualizations as URLs and embedded web widgets. Users can explore graphs\nincrementally by adding more related nodes, such as highly cited papers\ncited by or citing a paper of interest in a citation network. Argo Lite\nworks across devices and platforms, leveraging WebGL for high-performance\nrendering. Argo Lite has been used by over 1,000 students at Georgia Tech's\nData and Visual Analytics class. Argo Lite may serve as a valuable\nopen-source tool for advancing multiple CIKM research areas, from data\npresentation, to interfaces for information systems and more.",crownCaption:"Argo Lite visualizing a citation network of recent COVID-19 publications.\nArgo Lite users can explore graphs incrementally by adding more\nrelated papers (e.g., highly cited papers cited by or citing a paper of\ninterest) to the visualization. Using WebGL for high-performance\ncross-platform graph rendering, Argo Lite runs in all modern web\nbrowsers without requiring any installation.",bibtex:"@article{liArgoLiteOpenSource2020,\n  title={Argo Lite: Open-Source Interactive Graph Exploration and Visualization in Browsers},\n  author={Li, Siwei and Zhou, Zhiyan and Upadhayay, Anish and Shaikh, Omar and Freitas, Scott and Park, Haekyu and Wang, Zijie J and Routray, Susanta and Hull, Matthew and Chau, Duen Horng},\n  booktitle={Proceedings of the International Conference on Information and Knowledge Management},\n  year={2020},\n  organization={ACM}\n}"},{id:"people-map",title:"PeopleMap: Visualization Tool for Mapping Out Researchers using Natural Language Processing",authors:[R,m,a,"Austin P. Wright","Sasha Richardson",b],venue:"arXiv:2006.06105 [cs]",venueURL:S,year:e,url:"\u002Fpapers\u002Fpeople-map",pdf:S,repo:"poloclub\u002Fpeople-map",type:n,demo:"https:\u002F\u002Fpoloclub.github.io\u002Fpeople-map\u002Fideas\u002F",abstract:"Discovering research expertise at institutions can be a difficult task.\nManually curated university directories easily become out of date and they\noften lack the information necessary for understanding a researcher's\ninterests and past work, making it harder to explore the diversity of\nresearch at an institution and identify research talents. This results in\nlost opportunities for both internal and external entities to discover new\nconnections and nurture research collaboration. To solve this problem, we\nhave developed PeopleMap, the first interactive, open-source, web-based tool\nthat visually \"maps out\" researchers based on their research interests and\npublications by leveraging embeddings generated by natural language\nprocessing (NLP) techniques. PeopleMap provides a new engaging way for\ninstitutions to summarize their research talents and for people to discover\nnew connections. The platform is developed with ease-of-use and\nsustainability in mind. Using only researchers' Google Scholar profiles as\ninput, PeopleMap can be readily adopted by any institution using its\npublicly-accessible repository and detailed documentation.",crownCaption:"PeopleMap visually maps out researchers based on their research interests\nand publications. Here, a PeopleMap user is exploring the research topics of\nthe faculty members at the Institute of Data Engineering and Science (IDEaS)\nat Georgia Tech (https:\u002F\u002Fpoloclub.github.io\u002Fpeople-map\u002Fideas\u002F) A. Map View\nvisualizes the embedding of researchers generated using their research\ntopics and publication data, with each dot representing a researcher. B.\nResearch Query allows users to search for researchers and query areas of\nstudy, allowing the user to both locate specific individuals and see the\nresearchers most associated with a queried field in the Map View. C.\nResearcher View shows the detailed information (e.g., affiliation,\ncitations, interests) of a researcher highlighted in Map View. D. Control\nPanel allows users to adjust the hyperparameters of the Map View\nvisualization (e.g., show research names and cluster information).",bibtex:"@article{saad-falconPeopleMapVisualizationTool2020,\n  title = {{{PeopleMap}}: {{Visualization Tool}} for {{Mapping Out Researchers}} Using {{Natural Language Processing}}},\n  shorttitle = {{{PeopleMap}}},\n  author = {{Saad-Falcon}, Jon and Shaikh, Omar and Wang, Zijie J. and Wright, Austin P. and Richardson, Sasha and Chau, Duen Horng},\n  year = {2020},\n  month = jun,\n  archivePrefix = {arXiv},\n  eprint = {2006.06105},\n  eprinttype = {arxiv},\n  journal = {arXiv:2006.06105 [cs]},\n  primaryClass = {cs}\n}"},{id:T,title:"UnMask: Adversarial Detection and Defense Through Robust Feature Alignment",authors:[O,"Shang-Tse Chen",a,b],venue:"arXiv:22002.09576 [cs]",venueURL:U,year:e,url:"\u002Fpapers\u002Funmask",pdf:U,repo:"unmaskd\u002Funmask",type:n,abstract:"Deep learning models are being integrated into a wide range of high-impact,\nsecurity-critical systems, from self-driving cars to medical diagnosis.\nHowever, recent research has demonstrated that many of these deep learning\narchitectures are vulnerable to adversarial attacks--highlighting the vital\nneed for defensive techniques to detect and mitigate these attacks before\nthey occur. To combat these adversarial attacks, we developed UnMask, an\nadversarial detection and defense framework based on robust feature\nalignment. The core idea behind UnMask is to protect these models by\nverifying that an image's predicted class (\"bird\") contains the expected\nrobust features (e.g., beak, wings, eyes). For example, if an image is\nclassified as \"bird\", but the extracted features are wheel, saddle and\nframe, the model may be under attack. UnMask detects such attacks and\ndefends the model by rectifying the misclassification, re-classifying the\nimage based on its robust features. Our extensive evaluation shows that\nUnMask (1) detects up to 96.75% of attacks, with a false positive rate of\n9.66% and (2) defends the model by correctly classifying up to 93% of\nadversarial images produced by the current strongest attack, Projected\nGradient Descent, in the gray-box setting. UnMask provides significantly\nbetter protection than adversarial training across 8 attack vectors,\naveraging 31.18% higher accuracy. Our proposed method is architecture\nagnostic and fast. We open source the code repository and data with this\npaper: https:\u002F\u002Fgithub.com\u002Funmaskd\u002Funmask. ",crownShowBorder:f,crownCaption:"UnMask Framework Overview. UnMask combats adversarial attacks (in\nred) through extracting robust features from an image (“Bicycle” at top), and\ncomparing them to expected features of the classification (“Bird” at bottom)\nfrom the unprotected model. Low feature overlap signals an\nattack. UnMask rectifies misclassification using the image’s extracted\nfeatures. Our approach detects 96.75% of gray-box attacks (at 9.66% false\npositive rate) and defends the model by correctly classifying up to 93%\nof adversarial images crafted by Projected Gradient Descent (PGD).",bibtex:"@article{freitasUnMaskAdversarialDetection2020,\n  title = {{{UnMask}}: {{Adversarial Detection}} and {{Defense Through Robust Feature Alignment}}},\n  shorttitle = {{{UnMask}}},\n  author = {Freitas, Scott and Chen, Shang-Tse and Wang, Zijie J. and Chau, Duen Horng},\n  year = {2020},\n  archivePrefix = {arXiv},\n  eprint = {2002.09576},\n  eprinttype = {arxiv},\n  journal = {arXiv:2002.09576}\n}"},{id:"massif",title:"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning",authors:[j,h,a,r,L,M,b],equals:[j,h],venue:V,venueURL:"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3334480.3382977",venueShort:W,location:X,year:e,url:"\u002Fpapers\u002Fmassif",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2001.07769",type:n,abstract:"Deep neural networks (DNNs) are increasingly powering high-stakes\napplications such as autonomous cars and healthcare; however, DNNs are often\ntreated as \"black boxes\" in such applications. Recent research has also\nrevealed that DNNs are highly vulnerable to adversarial attacks, raising\nserious concerns over deploying DNNs in the real world. To overcome these\ndeficiencies, we are developing Massif, an interactive tool for deciphering\nadversarial attacks. Massif identifies and interactively visualizes neurons\nand their connections inside a DNN that are strongly activated or suppressed\nby an adversarial attack. Massif provides both a high-level, interpretable\noverview of the effect of an attack on a DNN, and a low-level, detailed\ndescription of the affected neurons. Massif's tightly coupled views help\npeople better understand which input features are most vulnerable and\nimportant for correct predictions.",crownCaption:"The MASSIF interface. A user Hailey is studying the targeted Fast Gradient\nMethod (FGM) attack performed on the InceptionV1 model. Using the control\npanel (A), she selects “giant panda” as the benign class and “armadillo” as\nthe attack target class. MASSIF generates an attribution graph (B), which\nshows Hailey the neurons within the network that are suppressed in the\nattacked images (B1, blue), shared by both benign and attacked images (B2,\npurple), and emphasized only in the attacked images (B3, orange). Each\nneuron is represented by a node and its feature visualization (C). Hovering\nover any neuron displays example dataset patches that maximally activate\nthe neuron, providing stronger evidence for what a neuron has learned to\ndetect. Hovering over a neuron also highlights its most influential\nconnections from the previous layer (D), allowing Hailey to determine where\nin the network the prediction diverges from the benign class to the attacked\nclass.",bibtex:"@inproceedings{das2020massif,\n  title={Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning},\n  author={Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  booktitle={Proceedings of the 2020 CHI Conference Extended Abstracts on Human Factors in Computing Systems},\n  publisher={ACM},\n  year={2020}\n}"},{id:"cnn-101",title:"CNN 101: Interactive Visual Learning for Convolutional Neural Networks",authors:[a,u,m,h,j,r,J,b],venue:V,venueShort:W,location:X,venueURL:"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3334480.3382899",year:e,url:"\u002Fpapers\u002Fcnn-101",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2001.02004",type:n,video:"https:\u002F\u002Fyoutu.be\u002Fg082-zitM7s",crownCaption:"The Overview (A) visualizes activation maps of all neurons as heatmaps\nconnected with edges. When user clicks a convolutional neuron in (A), the\nview transitions to the Convolutional Intermediate View (A=\u003EB).The Flatten\nIntermediate View appears when an output neuron is selected instead\n(A=\u003EC). (B) demonstrates the relationship between selected convolutional\nneuron and its previous layer. (B) transitions to the Detail View which illustrates\nthe convolution operation on selected input neuron (B=\u003ED). (C) explains\nthe flatten layer between the second last layer and output layer.",abstract:"The success of deep learning solving previously-thought hard problems has\ninspired many non-experts to learn and understand this exciting technology.\nHowever, it is often challenging for learners to take the first steps due to\nthe complexity of deep learning models. We present our ongoing work, CNN\n101, an interactive visualization system for explaining and teaching\nconvolutional neural networks. Through tightly integrated interactive views,\nCNN 101 offers both overview and detailed descriptions of how a model works.\nBuilt using modern web technologies, CNN 101 runs locally in users' web\nbrowsers without requiring specialized hardware, broadening the public's\neducation access to modern deep learning techniques. ",bibtex:"@inproceedings{wangCNN101Interactive2020,\n  title = {{{CNN}} 101: {{Interactive}} Visual Learning for Convolutional Neural Networks},\n  booktitle = {Extended Abstracts of the 2020 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  year = {2020},\n  publisher = {{ACM}},\n  place = {{Honolulu, HI, USA}}\n}"},{id:"t-cell",title:"Classifying T cell activity in autofluorescence intensity images with convolutional neural networks",authors:[a,v,w,l],venue:"Journal of Biophotonics",venueShort:"J. Biophotonics",venueURL:"https:\u002F\u002Fonlinelibrary.wiley.com\u002Fjournal\u002F18640648",year:Y,url:"\u002Fpapers\u002Ft-cell",pdf:"https:\u002F\u002Fonlinelibrary.wiley.com\u002Fdoi\u002Fepdf\u002F10.1002\u002Fjbio.201960050",slides:"\u002Fslides\u002Fhonor_thesis_symposium_2019.pdf",repo:"gitter-lab\u002Ft-cell-classification",showStar:"flase",data:"https:\u002F\u002Fdoi.org\u002F10.5281\u002Fzenodo.2640835",type:"journal",crownShowBorder:f,crownCaption:"Our T cell image data processing workflow.",abstract:"The importance of T cells in immunotherapy has motivated developing\ntechnologies to better characterize T cells and improve therapeutic\nefficacy. One specific objective is assessing antigen-induced T cell\nactivation because only functionally active T cells are capable of killing\nthe desired targets. Autofluorescence imaging can distinguish T cell\nactivity states of individual cells in a non-destructive manner by detecting\nendogenous changes in metabolic co-enzymes such as NAD(P)H. However,\nrecognizing robust patterns of T cell activity is computationally\nchallenging in the absence of exogenous labels or information-rich\nautofluorescence lifetime measurements. We demonstrate that advanced machine\nlearning can accurately classify T cell activity from NAD(P)H intensity\nimages and that those image-based signatures transfer across human donors.\nUsing a dataset of 8,260 cropped single-cell images from six donors, we\nmeticulously evaluate multiple machine learning models. These range from\ntraditional models that represent images using summary statistics or extract\nimage features with CellProfiler to deep convolutional neural networks\n(CNNs) pre-trained on general non-biological images. Adapting pre-trained\nCNNs for the T cell activity classification task provides substantially\nbetter performance than traditional models or a simple CNN trained with the\nautofluorescence images alone. Visualizing the images with dimension\nreduction provides intuition into why the CNNs achieve higher accuracy than\nother approaches. However, we observe that fine-tuning all layers of the\npre-trained CNN does not provide a classification performance boost\ncommensurate with the additional computational cost. Our software detailing\nour image processing and model training pipeline is available as Jupyter\nnotebooks at https:\u002F\u002Fgithub.com\u002Fgitter-lab\u002Ft-cell-classification.",bibtex:"@article{wang_classifying_2019,\n  title = {Classifying {T} cell activity in autofluorescence intensity images with convolutional neural networks},\n  issn = {1864-063X, 1864-0648},\n  url = {https:\u002F\u002Fonlinelibrary.wiley.com\u002Fdoi\u002Fabs\u002F10.1002\u002Fjbio.201960050},\n  doi = {10.1002\u002Fjbio.201960050},\n  language = {en},\n  urldate = {2020-01-12},\n  journal = {Journal of Biophotonics},\n  author = {Wang, Zijie J. and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  month = dec,\n  year = {2019}\n}"},{id:"t-cell-poster",title:Z,authors:[a,v,w,l],venue:"International Society for Computational Biology Great Lakes Bioinformatics Conference",venueShort:"ISCB GLBIO",venueURL:"https:\u002F\u002Fwww.iscb.org\u002Fglbio2019",location:"Madison, WI, USA",year:Y,url:"\u002Fpapers\u002Ft-cell-poster",pdf:"\u002Fpdf\u002F19-tcell-glbio.pdf",code:"https:\u002F\u002Fgithub.com\u002Fgitter-lab\u002Ft-cell-classification",type:_,crownShowBorder:f,crownCaption:"Poster presented at the International Society for Computational Biology\nGreat Lakes Bioinformatics Conference (ISCB GLBIO).",abstract:"T cell activity state is an important component of immunotherapy efficacy in\nclinical cancer treatment. However, current image-based activity profiling\nmethods destroy cells and require exogenous contrast agents, making them\nunsuitable for clinical applications. In this study, we use non-destructive,\nT cell autofluorescence microscopy images to measure NAD(P)H intensity and\nclassify individual T cells as activated or quiescent. We assess five\nmachine learning methods of increasing complexity, ranging from linear\nclassifiers to deep convolutional neural networks pre-trained on generic\nimages. To evaluate these models and determine whether they are accurate\nacross different human T cell donors, we designed a meticulous nested\ncross-validation scheme to tune and test each model. A retrained\nconvolutional neural network, the most advanced model, achieved an average\naccuracy of 91.4% when classifying quiescent and activated T cells.\nImportantly, it gave 98% accuracy on an independent donor that was held out\nuntil all aspects of the training and tuning procedures were finalized. This\nshows that autofluorescence microscopy with a state-of-the-art image\nclassification algorithm is a powerful tool for label-free and\nnon-destructive assessment of T cell activity state, even when only NAD(P)H\nintensity is provided as the input feature. In addition, our high-throughput\nhyperparameter selection results give empirical insights on practical deep\nlearning deployment with microscopy image data. Similarly, the model\ncomparisons examined the tradeoff between performance and model complexity,\nwhich provides alternative methods that are suitable when computing resource\nare limited. We observe that retraining more layers in a pre-trained\nconvolutional neural network does not bring performance improvements that\njustify the high computational costs. Finally, we are preparing all of our\ncode in Jupyter notebooks with reproducible examples of image processing and\nclassification. These comprehensive notebooks serve as an instructional tool\nfor readers who are not familiar with machine learning application on\nmicroscopy images.",bibtex:"@inproceedings{wang_classifying_poster_2019,\n  title = {Classifying {T} cell activity with convolutional neural networks},\n  language = {en},\n  conference = {International Society for Computational Biology Great Lakes Bioinformatics Conference},\n  author = {Wang, Zijie J. and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  year = {2019}\n}"},{id:"breast-poster",title:"Using Transfer Learning to Classify Breast Cancer Cells with Fluorescence Imaging",authors:[a,"Tiffany M. Heaster","Quan Yin",v,w,l],venue:"University of Wisconsin–Madison Undergraduate Symposium","venue-short":"",venueURL:"https:\u002F\u002Fugradsymposium.wisc.edu",year:2018,url:"\u002Fpapers\u002Fbreast-poster",pdf:"\u002Fpdf\u002F18-breast-symposium.pdf",type:_,abstract:"Studying tumor heterogeneity by analyzing protein or gene expression levels\nover thousands of cells is very challenging. In this project, we instead use\na transfer learning approach to classify cancer cell types solely based on\nfluorescence imaging. We used images of two types of breast cancer cell\nlines – MDA-MB-231 and SKBr3 – to partially retrain a deep convolutional\nneural network Inception v3, which was pre-trained on 10 million natural\nimages with over 400 categories. We hypothesize features extracted from\ngeneral pictures by a deep neural network are portable to classify breast\ncancer cell types. The ability to recognize distinct cell types within\ntumors would provide a powerful tool for analyzing clinical samples.",crownCaption:"Poster presented at the University of Wisconsin–Madison Undergraduate Symposium.",crownShowBorder:f,bibtex:"@inproceedings{wang_using_poster_2018,\n  title = {Using Transfer Learning to Classify Breast Cancer Cells with Fluorescence Imaging},\n  language = {en},\n  conference = {University of Wisconsin–Madison Undergraduate Symposium},\n  author = {Wang, Zijie J. and Heaster, Tiffany M. and Yin, Quan and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  year = {2018}\n}"}],talk:[{name:Z,events:[{time:"April 2019",place:"UW–Madison Senior Honors Thesis Symposium",url:"https:\u002F\u002Fhonors.ls.wisc.edu\u002Fwp-content\u002Fuploads\u002Fsites\u002F1038\u002F2019\u002F04\u002FSymposiumSchedule2019email.pdf"}]}],award:[{name:"Dean's List",description:"Achieved at least a 3.60 GPA as freshmen and sophomores, a 3.85 GPA as juniors and seniors",time:"Aug. 2015 – May 2019"},{name:"University Book Store Academic Excellence Award ($1000)",description:"An award recognizing undergraduate students who have completed an outstanding independent project, such as a senior thesis, at the University of Wisconsin–Madison",time:o},{name:"Honors Senior Thesis Summer Research Grant ($3000)",description:"A research grant funding students to undertake more demanding and extensive senior thesis research projects",time:"June 2018"},{name:"Welton Summer Sophomore Apprenticeship ($2500)",description:"A research grant awarded to talented students to participate in actual, cutting-edge research",time:"June 2017"}],teaching:[{title:"Undergraduate Teaching Assistant",school:c,schoolURL:s,course:"Computer Graphics (CS 559)",courseURL:"https:\u002F\u002Fgraphics.cs.wisc.edu\u002FWP\u002Fcs559-sp2019\u002Foverview\u002F",instructor:p,timeStart:G,timeEnd:o,place:d,description:"Created course notes and weekly assignments, held weekly office hours, and answered student questions on Piazza. The course had 180 undergraduates enrolled.\n"},{title:"Notetaker",school:c,schoolURL:s,course:"McBurney Disability Resource Center",courseURL:"https:\u002F\u002Fmcburney.wisc.edu\u002F",timeStart:"Sep. 2016",timeEnd:o,place:d,description:"Provided clearly-written math and statistics notes to students with disability, answered course-related questions.\n"},{title:"Academic Coach",school:c,schoolURL:s,course:"Division of Diversity, Equity and Educational Achievement",courseURL:"https:\u002F\u002Fdiversity.wisc.edu\u002Fabout\u002Fabout-ddeea\u002F",timeStart:"Nov. 2016",timeEnd:"May 2017",place:d,description:"Mentored undergraduate students in DDEEA programs for Data Structure course, designed two worksheets and provided detailed solutions every week.\n"},{title:"Tutor",school:c,schoolURL:s,course:"Greater University Tutoring Service",courseURL:"https:\u002F\u002Fguts.wisc.edu\u002F",timeStart:"Jan. 2016",timeEnd:"Jan. 2017",place:d,description:"Instructed peers one-on-one in programming and math problems for three hours weekly, led review sections to help students study for calculus exams.\n"}],mentoring:[{name:R,timeStart:"May 2020",timeEnd:g,degree:x},{name:u,timeStart:k,timeEnd:g,degree:x,description:"Machine learning and visualization",awards:["PURA Travel Award (2020)"]},{name:m,timeStart:k,timeEnd:g,degree:x,awards:["Outstanding Freshman Award (2020)"]}],service:{review:[{venue:"IEEE Visual Analytics Science and Technology",venueShort:"VAST",years:[{year:e,yearURL:N}]},{venue:"ACM Conference on Information and Knowledge Management",venueShort:P,years:[{year:e,yearURL:Q}]}],membership:[{org:"Institute of Electrical and Electronics Engineers",orgShort:"IEEE",orgURL:"https:\u002F\u002Fwww.ieee.org\u002F",timeStart:"July 2019",timeEnd:g},{org:"Association for Computing Machinery",orgShort:"ACM",orgURL:"https:\u002F\u002Fwww.acm.org\u002F",timeStart:"Dec. 2019",timeEnd:g}]},reference:[{name:"Polo Chau",position:"Associate Professor",department:C,departmentURL:$,institution:t,institutionURL:$,url:"https:\u002F\u002Fcc.gatech.edu\u002F~dchau\u002F"},{name:"Anghony Gitter",position:"Assistant Professor",department:"Department of Biostatistics and Medical Informatics",departmentURL:"https:\u002F\u002Fwww.biostat.wisc.edu\u002F",institution:c,institutionURL:i,url:"https:\u002F\u002Fwww.biostat.wisc.edu\u002F~gitter\u002F"},{name:p,position:"Professor",department:F,departmentURL:"https:\u002F\u002Fwww.cs.wisc.edu\u002F",institution:c,institutionURL:i,url:z}],skill:[{group:"Programming",items:["Python","JavaScript","Swift","R","Julia","PyTorch","TensorFlow","Keras","HTML","CSS","LaTeX","SQL","C++","Git"]},{group:"Design",items:["Affinity Designer","Affinity Photo","Final Cut Pro","Sketch","Keynote","Illustrator","Photoshop"]},{group:"HCI",items:["Think-aloud protocol","User Personas","Rapid Paper Prototyping","Affinity Diagraming"]}],news:[{date:"Aug. 14, 2020",news:"My paper \u003Ca href='papers\u002Fcnn-explainer' target='_self'\u003ECNN Explainer\u003C\u002Fa\u003E is accepted for IEEE VIS 2020 VAST (TVCG Journal Track)!\n"},{date:"June 18, 2020",news:"Started my first internship at \u003Ca href='https:\u002F\u002Fwww.bosch.us\u002Four-company\u002Fbosch-in-the-usa\u002Fsunnyvale\u002F' target='_self'\u003EBosch (Sunnyvale)\u003C\u002Fa\u003E , working with \u003Ca href='https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Flianggou\u002F' target='_self'\u003ELiang Gou\u003C\u002Fa\u003E on visual analytics for autonomous driving.\n"},{date:"April 30, 2020",news:"Posted my \u003Ca href='https:\u002F\u002Farxiv.org\u002Fabs\u002F2004.15004' target='_self'\u003ECNN Explainer paper\u003C\u002Fa\u003E on arXiv. CNN Explainer an \u003Ca href='http:\u002F\u002Fpoloclub.github.io\u002Fcnn-explainer\u002F' target='_self'\u003Einteractive tool\u003C\u002Fa\u003E that helps beginners learn CNNs. It is also \u003Ca href='https:\u002F\u002Fgithub.com\u002Fpoloclub\u002Fcnn-explainer' target='_self'\u003Eopen-sourced\u003C\u002Fa\u003E on GitHub.\n"},{date:"Jan 15, 2020",news:"Two papers, \u003Ca href='papers\u002Fcnn-101' target='_self'\u003ECNN 101\u003C\u002Fa\u003E and \u003Ca href='papers\u002Fmassif' target='_self'\u003EMassif\u003C\u002Fa\u003E, are accepted for CHI 2020 Late-Breaking Works!\n"},{date:"Nov. 15, 2019",news:"I will present my T Cell Classification \u003Ca href='\u002Fpdf\u002Fglbio_2019.pdf' target='_self'\u003Eposter\u003C\u002Fa\u003E at \u003Ca href='https:\u002F\u002Fostem.org\u002Fpage\u002Fconference-2019' target='_self'\u003EoSTEM'19\u003C\u002Fa\u003E. See you in Detroit (really miss the Midwest winter!). \n"},{date:"Oct. 1, 2019",news:"I will attend \u003Ca href='http:\u002F\u002Fieeevis.org\u002Fyear\u002F2019\u002Fwelcome' target='_self'\u003EVIS'19\u003C\u002Fa\u003E in Vancouver. Come to talk to me :) \n"},{date:"Aug. 15, 2019",news:"Submitted my first \u003Ca href='https:\u002F\u002Fwww.biorxiv.org\u002Fcontent\u002F10.1101\u002F737346' target='_self'\u003Epaper\u003C\u002Fa\u003E on bioRxiv. Check it out!\n"},{date:"May. 11, 2019",news:"I \u003Ca href='https:\u002F\u002Fwww.biorxiv.org\u002Fcontent\u002F10.1101\u002F737346' target='_self'\u003Egraduated\u003C\u002Fa\u003E! Will always be a proud badger!! 🦡\n"},{date:"April 20, 2019",news:"Honored to receive the 2019 \u003Ca href='https:\u002F\u002Fawards.advising.wisc.edu\u002Fcampus-wide-award-recipients\u002F2016-university-book-store-award-recipients\u002F' target='_self'\u003E University Book Store Academic Excellence Award\u003C\u002Fa\u003E.\n"},{date:"April 10, 2019",news:"Excited to present my T-cell classification project as a poster in     \u003Ca href='https:\u002F\u002Fwww.iscb.org\u002Fglbio2019' target='_self'\u003EGLBIO'19\u003C\u002Fa\u003E.\n"}],featured:[{id:I,featureImg:"\u002Fimages\u002Fteasers\u002Fcnn-explainer.png"},{id:K,featureImg:"\u002Fimages\u002Fteasers\u002Fbluff.png"},{id:T,featureImg:"\u002Fimages\u002Fteasers\u002Funmask.png"}],project:[{name:"Clip2imgur",repo:"xiaohk\u002Fclip2imgur",teaser:"\u002Fimages\u002Fteasers\u002Fproject-clip2imgur.png",award:{name:"Featured on Imgur",url:"https:\u002F\u002Fhelp.imgur.com\u002Fhc\u002Fen-us\u002Farticles\u002F209592766-Tools-for-Imgur"},description:"Convenient macOS command line tool for uploading screen-shots from the clipboard to Imgur. \n"},{name:"FaceData",repo:"xiaohk\u002FFaceData",teaser:"\u002Fimages\u002Fteasers\u002Fproject-face.jpg",description:"MacOS GUI to auto-annotate facial landmarks from a video. Landmarks can be used to train GANs.\n"},{name:"Graphics on the Web",repo:aa,demo:"http:\u002F\u002Fjayw-www.cs.wisc.edu\u002Fcs559\u002Fp10\u002F",teaser:"\u002Fimages\u002Fteasers\u002Fproject-graphics.png",description:"Interactive 2D, 2.5D and 3D computational graphics with shaders and textures, created with HTML canvas and webGL.\n"},{name:"Group Assignment Problem",repo:aa,demo:"https:\u002F\u002Fnbviewer.jupyter.org\u002Fgithub\u002Fxiaohk\u002FCS524-Group-Assignment-Optimization\u002Fblob\u002Fmaster\u002FWang.ipynb",teaser:"\u002Fimages\u002Fteasers\u002Fproject-optim.png",award:{name:"Best project",url:"https:\u002F\u002Flaurentlessard.com\u002Fteaching\u002F524-intro-to-optimization\u002F"},description:"Flexible and robust Mixed Integer Quadratic Programming model written in Julia to solve a real-life optimization problem.\n"},{name:"Dean's list Vis",demo:"http:\u002F\u002Fjayw-www.cs.wisc.edu\u002Fd3-china-map\u002F",repo:"xiaohk\u002Fd3-china-map",teaser:"\u002Fimages\u002Fteasers\u002Fproject-map.png",description:"Interactive geo-visualization to explore where UW–Madison Chinese students are from.\n"},{name:"Yelp Sentiment",repo:"xiaohk\u002Fstat333_project_2",teaser:"\u002Fimages\u002Fteasers\u002Fproject-review.png",award:{name:"In-class Kaggle winner",url:"https:\u002F\u002Fwww.kaggle.com\u002Fc\u002Fuw-madison-sp17-stat333"},description:"Predicting Yelp ratings based on text comments of Madison restaurants.\n"}]}}}("Zijie J. Wang","Duen Horng (Polo) Chau","University of Wisconsin–Madison","Madison, WI",2020,true,"Present","Haekyu Park","https:\u002F\u002Fwww.wisc.edu","Nilaksh Das","Aug. 2019","Anthony Gitter","Omar Shaikh","arxiv","May 2019","Michael Gleicher","academic","Fred Hohman","https:\u002F\u002Fwww.wisc.edu\u002F","Georgia Institute of Technology","Robert Turko","Alex J. Walsh","Melissa C. Skala","B.S. in Computer Science, Georgia Institute of Technology","https:\u002F\u002Fwww.cc.gatech.edu\u002F~dchau","http:\u002F\u002Fpages.cs.wisc.edu\u002F~gleicher\u002F","Atlanta, GA","Yu Hen Hu","School of Computational Science and Engineering","https:\u002F\u002Fwww.cs.wisc.edu","Undergraduate Researcher","Department of Computer Sciences","Dec. 2018","Dec. 2017","cnn-explainer","Minsuk Kahng","bluff","Robert Firstman","Emily Rogers","http:\u002F\u002Fieeevis.org\u002Fyear\u002F2020\u002Fwelcome","Scott Freitas","CIKM","https:\u002F\u002Fwww.cikm2020.org\u002F","Jon Saad-Falcon","https:\u002F\u002Farxiv.org\u002Fabs\u002F2006.06105","unmask","https:\u002F\u002Farxiv.org\u002Fabs\u002F2002.09576","Extended Abstracts on ACM Human Factors in Computing Systems","CHI","Honolulu, HI, USA",2019,"Classifying T cell activity with convolutional neural networks","poster",null,"xiaohk\u002FCS559-computational-graphics"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.eff93043.js"}catch(e){main="/client/legacy/client.3bcdc8ca.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 