<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest> <link href=favicon.ico rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Roboto+Mono:ital,wght@0,100;0,300;0,400;0,500;0,700;1,100;1,300;1,400;1,500;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet> <link href=client/main.3754935911.css rel=stylesheet><link href=client/index.88568c4a.css rel=stylesheet><link href=client/client.7ddfe460.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Jay Wang</title><meta content="Jay Wang" class=svelte-cyrvy1 name=title><meta content="Zijie Jay Wang is a Ph.D. student in the College of Computing at Georgia Tech researching the intersection of machine learning and data visualization." class=svelte-cyrvy1 name=description><meta content=website class=svelte-cyrvy1 property=og:type><meta content=https://zijie.wang/ class=svelte-cyrvy1 property=og:url><meta content="Jay Wang" class=svelte-cyrvy1 property=og:title><meta content="Zijie Jay Wang is a Ph.D. student in the College of Computing at Georgia Tech researching the intersection of machine learning and data visualization." class=svelte-cyrvy1 property=og:description><meta content=https://zijie.wang/images/teasers/preview.png class=svelte-cyrvy1 property=og:image><meta content=summary_large_image class=svelte-cyrvy1 property=twitter:card><meta content=https://zijie.wang/ class=svelte-cyrvy1 property=twitter:url><meta content="Jay Wang" class=svelte-cyrvy1 property=twitter:title><meta content="Zijie Jay Wang is a Ph.D. student in the College of Computing at Georgia Tech researching the intersection of machine learning and data visualization." class=svelte-cyrvy1 property=twitter:description><meta content=https://zijie.wang/images/teasers/preview.png class=svelte-cyrvy1 property=twitter:image><meta content=@jay4w class=svelte-cyrvy1 property=twitter:site><meta content=@jay4w class=svelte-cyrvy1 property=twitter:creator><script class=svelte-cyrvy1 async src="https://www.googletagmanager.com/gtag/js?id=UA-130177683-1"></script><script class=svelte-cyrvy1>window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-130177683-1'); </script><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <main class=svelte-1jybnva> <div class="svelte-cyrvy1 page"><div class="svelte-cyrvy1 left-padding"></div> <div class="svelte-cyrvy1 content"> <div class="svelte-cyrvy1 block-tb"><h1 class="svelte-cyrvy1 header name">Zijie Jay Wang</h1> <div class="svelte-cyrvy1 block-lr"><div class="svelte-cyrvy1 name-text"><p class=svelte-cyrvy1>Hi, I'm Jay. I'm a Machine Learning Ph.D. student in the College of Computing at <a href=https://gatech.edu/ target=_self>Georgia Tech</a> adviced by <a href=https://www.cc.gatech.edu/~dchau target=_self>Polo Chau</a>. </p> <p class=svelte-cyrvy1>My research focuses on making AI more accessible, interpretable, and accountable, by designing and developing novel <span class="svelte-cyrvy1 strong">interactive interfaces</span> for people to easily and enjoyably interact with <span class="svelte-cyrvy1 strong">machine learning systems</span> at scale. </p> <p class=svelte-cyrvy1>I recieved my B.S. from <a href=https://wisc.edu target=_self>UW–Madison</a>, where I worked closely with <a href=https://www.biostat.wisc.edu/~gitter target=_self>Anthony Gitter</a>, <a href=http://pages.cs.wisc.edu/~gleicher/ target=_self>Michael Gleicher</a>, and <a href=http://homepages.cae.wisc.edu/~hu/ target=_self>Yu Hen Hu</a>. </div> <div class="svelte-cyrvy1 name-pic"><img alt="Jay Wang" class=svelte-cyrvy1 src=images/jay.jpg> <div class="svelte-cyrvy1 block-tb" style="padding:0 5px"><div class="svelte-cyrvy1 icons"><a href=https://github.com/xiaohk target=_self class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></a> <a href="https://scholar.google.com/citations?user=eouAYvcAAAAJ&hl=en" target=_self class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#google-scholar></use></svg></a> <a href=https://orcid.org/0000-0003-4360-1423 target=_self class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#orcid-brands></use></svg></a> <a href=https://twitter.com/Jay4w target=_self class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#twitter-brands></use></svg></a> <a href=https://www.linkedin.com/in/zijiewang/ target=_self class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#linkedin-brands></use></svg></a></div> <div class="svelte-cyrvy1 infos"><div class="svelte-cyrvy1 label-text" style="font-family:Roboto Mono"><a href=mailto:jayw@gatech.edu target=_self class=svelte-cyrvy1>jayw@gatech.edu</a></div> <div class="svelte-cyrvy1 label-cv"><a href=/cv target=_self class=svelte-cyrvy1 rel=prefetch>CV</a></div></div></div></div></div> <div class="svelte-cyrvy1 has-block-name news-block"><div class="svelte-cyrvy1 block-name">News</div> <div class="svelte-cyrvy1 news-content"><div class="svelte-cyrvy1 news"><div class="svelte-cyrvy1 news-date">Aug. 29, 2020</div> <div class="svelte-cyrvy1 news-text">Two papers, <a href=papers/cnn-explainer>CNN Explainer</a> and <a href=papers/bluff>Bluff</a>, are accepted for IEEE VIS 2020!</div> </div><div class="svelte-cyrvy1 news"><div class="svelte-cyrvy1 news-date">June 18, 2020</div> <div class="svelte-cyrvy1 news-text">Started my first internship at <a href=https://www.bosch.us/our-company/bosch-in-the-usa/sunnyvale/ target=_self>Bosch (Sunnyvale)</a> , working with <a href=https://www.linkedin.com/in/lianggou/ target=_self>Liang Gou</a> on visual analytics for autonomous driving. </div> </div><div class="svelte-cyrvy1 news"><div class="svelte-cyrvy1 news-date">April 30, 2020</div> <div class="svelte-cyrvy1 news-text">Posted <a href=https://arxiv.org/abs/2004.15004 target=_self>CNN Explainer paper</a> on arXiv. CNN Explainer an <a href=http://poloclub.github.io/cnn-explainer/ target=_self>interactive tool</a> that helps beginners learn CNNs. It is also <a href=https://github.com/poloclub/cnn-explainer target=_self>open-sourced</a> on GitHub. </div> </div><div class="svelte-cyrvy1 news"><div class="svelte-cyrvy1 news-date">Jan 15, 2020</div> <div class="svelte-cyrvy1 news-text">Two papers, <a href=papers/cnn-101 target=_self>CNN 101</a> and <a href=papers/massif target=_self>Massif</a>, are accepted for CHI 2020 Late-Breaking Works! </div> </div></div> <div class="svelte-cyrvy1 svg-icon right-margin add-more-button"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#plus-circle-solid></use></svg> show more</div></div> <div class="svelte-cyrvy1 pub-block has-block-name"><div class="svelte-cyrvy1 block-name">Featured Publications</div> <div class="svelte-cyrvy1 news-content"><div class="svelte-cyrvy1 pub"><a href=/papers/cnn-explainer target=_self><img alt=Thumbnail class=svelte-cyrvy1 src=/images/teasers/cnn-explainer.png></a> <div class="svelte-cyrvy1 pub-content"><div class="svelte-cyrvy1 pub-title"><a href=/papers/cnn-explainer target=_self rel=prefetch>CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization </a></div> <div class="svelte-cyrvy1 pub-author"><a href=https://zijie.wang target=_self style=font-weight:700>Zijie J. Wang</a>, <a href=https://www.linkedin.com/in/robert-turko/ target=_self>Robert Turko</a>, <a href=http://oshaikh.com/ target=_self>Omar Shaikh</a>, <a href=https://haekyu.com target=_self>Haekyu Park</a>, <a href=https://nilakshdas.com target=_self>Nilaksh Das</a>, <a href=https://fredhohman.com target=_self>Fred Hohman</a>, <a href=https://minsuk.com target=_self>Minsuk Kahng</a>, <a href=https://www.cc.gatech.edu/~dchau target=_self>Duen Horng (Polo) Chau</a></div> <div class="svelte-cyrvy1 pub-venue"><a href=https://www.computer.org/csdl/journal/tg target=_self>IEEE Transactions on Visualization and Computer Graphics (TVCG), 2021 </a></div> <div class="svelte-cyrvy1 pub-icons"><a href=/papers/cnn-explainer target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#home-sharp></use></svg></div> <span class=svelte-cyrvy1>Project</span> </a> <a href=https://poloclub.github.io/cnn-explainer/ target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#rocket-sharp></use></svg></div> <span class=svelte-cyrvy1>Demo</span> </a> <a href=https://youtu.be/HnWIHWFbuUQ target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#play-solid></use></svg></div> <span class=svelte-cyrvy1>Video</span> </a> <a href=https://arxiv.org/abs/2004.15004 target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-pdf-solid></use></svg></div> <span class=svelte-cyrvy1>PDF</span> </a> <div class="svelte-cyrvy1 icon-container"><a href=https://github.com/poloclub/cnn-explainer target=_self class="svelte-cyrvy1 icon-container no-right-margin"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> <div class="svelte-cyrvy1 star-container"><div style=display:flex>(<a href=https://github.com/poloclub/cnn-explainer/stargazers target=_self class="svelte-cyrvy1 svg-icon" style=font-weight:500;margin-right:-3px> loading </a>)</div> </div> </div> <div class="svelte-cyrvy1 icon-container bibtex-button"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#quote-right-solid></use></svg></div> <span class=svelte-cyrvy1>BibTeX</span> </div> <div class="svelte-cyrvy1 icon-container award"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#award-solid></use></svg></div> <div class="svelte-cyrvy1 award-highlight"><a href=https://web.archive.org/web/20200505121955/https://github.com/trending target=_self>Top of GitHub Trending</a></div> </div></div> </div></div> <div class="svelte-cyrvy1 bibtex hidden"><pre class=svelte-cyrvy1>@article{wangCNNExplainerLearning2021,
  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},
  shorttitle = {{{CNN Explainer}}},
  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},
  journal={IEEE Transactions on Visualization and Computer Graphics (TVCG)},
  publisher={IEEE},
  year={2021},
}</pre> </div></div> </div><div class="svelte-cyrvy1 pub-block"> <div class="svelte-cyrvy1 news-content"><div class="svelte-cyrvy1 pub"><a href=/papers/bluff target=_self><img alt=Thumbnail class=svelte-cyrvy1 src=/images/teasers/bluff.png></a> <div class="svelte-cyrvy1 pub-content"><div class="svelte-cyrvy1 pub-title"><a href=/papers/bluff target=_self rel=prefetch>Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks </a></div> <div class="svelte-cyrvy1 pub-author"><a href=https://nilakshdas.com target=_self>Nilaksh Das</a>*, <a href=https://haekyu.com target=_self>Haekyu Park</a>*, <a href=https://zijie.wang target=_self style=font-weight:700>Zijie J. Wang</a>, <a href=https://fredhohman.com target=_self>Fred Hohman</a>, <a href=https://www.robfirstman.com/ target=_self>Robert Firstman</a>, <a href=https://www.linkedin.com/in/emily-rogers-1a828598 target=_self>Emily Rogers</a>, <a href=https://www.cc.gatech.edu/~dchau target=_self>Duen Horng (Polo) Chau</a></div> <div class="svelte-cyrvy1 icon-container comment">(*Authors contributed equally)</div> <div class="svelte-cyrvy1 pub-venue"><a href=http://ieeevis.org/year/2020/welcome target=_self>IEEE Visualization Conference (VIS), 2020 </a></div> <div class="svelte-cyrvy1 pub-icons"><a href=/papers/bluff target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#home-sharp></use></svg></div> <span class=svelte-cyrvy1>Project</span> </a> <a href=https://poloclub.github.io/bluff/ target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#rocket-sharp></use></svg></div> <span class=svelte-cyrvy1>Demo</span> </a> <a href=https://arxiv.org/abs/2009.02608 target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-pdf-solid></use></svg></div> <span class=svelte-cyrvy1>PDF</span> </a> <div class="svelte-cyrvy1 icon-container"><a href=https://github.com/poloclub/bluff target=_self class="svelte-cyrvy1 icon-container no-right-margin"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> </div> <div class="svelte-cyrvy1 icon-container bibtex-button"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#quote-right-solid></use></svg></div> <span class=svelte-cyrvy1>BibTeX</span> </div> </div> </div></div> <div class="svelte-cyrvy1 bibtex hidden"><pre class=svelte-cyrvy1>@article{dasBluffInteractivelyDeciphering2020,
  title={Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks},
  author={Das, Nilaksh and Park, Haekyu and Wang, Zijie J and Hohman, Fred and Firstman, Robert and Rogers, Emily and Chau, Duen Horng},
  booktitle={IEEE Visualization Conference (VIS)},
  publisher={IEEE},
  year={2020}
}</pre> </div></div> </div><div class="svelte-cyrvy1 pub-block"> <div class="svelte-cyrvy1 news-content"><div class="svelte-cyrvy1 pub"><a href=/papers/skeleton-vis target=_self><img alt=Thumbnail class=svelte-cyrvy1 src=/images/teasers/skeleton-vis.png></a> <div class="svelte-cyrvy1 pub-content"><div class="svelte-cyrvy1 pub-title"><a href=/papers/skeleton-vis target=_self rel=prefetch>SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models </a></div> <div class="svelte-cyrvy1 pub-author"><a href=https://haekyu.com target=_self>Haekyu Park</a>, <a href=https://zijie.wang target=_self style=font-weight:700>Zijie J. Wang</a>, <a href=https://nilakshdas.com target=_self>Nilaksh Das</a>, <a href=https://www.linkedin.com/in/anindyasankar/ target=_self>Anindya S. Paul</a>, <a href=https://www.linkedin.com/in/pruthvi-perumalla/ target=_self>Pruthvi Perumalla</a>, <a href=https://www.linkedin.com/in/frank-zhou-b19515159/ target=_self>Zhiyan Zhou</a>, <a href=https://www.cc.gatech.edu/~dchau target=_self>Duen Horng (Polo) Chau</a></div> <div class="svelte-cyrvy1 pub-venue"><a href=https://aaai.org/Conferences/AAAI-21/aaai21demoscall/ target=_self>AAAI Conference on Artificial Intelligence Demo (AAAI), 2021 </a></div> <div class="svelte-cyrvy1 pub-icons"><a href=/papers/skeleton-vis target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#home-sharp></use></svg></div> <span class=svelte-cyrvy1>Project</span> </a> <a href=https://poloclub.github.io/skeleton-vis/ target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#rocket-sharp></use></svg></div> <span class=svelte-cyrvy1>Demo</span> </a> <a href=https://youtu.be/xgK9maDqhi4 target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#play-solid></use></svg></div> <span class=svelte-cyrvy1>Video</span> </a> <a href=https://arxiv.org/abs/2101.10586 target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-pdf-solid></use></svg></div> <span class=svelte-cyrvy1>PDF</span> </a> <div class="svelte-cyrvy1 icon-container"><a href=https://github.com/poloclub/skeleton-vis target=_self class="svelte-cyrvy1 icon-container no-right-margin"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> </div> <div class="svelte-cyrvy1 icon-container bibtex-button"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#quote-right-solid></use></svg></div> <span class=svelte-cyrvy1>BibTeX</span> </div> </div> </div></div> <div class="svelte-cyrvy1 bibtex hidden"><pre class=svelte-cyrvy1>@article{park2021skeletonvis,
  title={SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models},
  author={Park, Haekyu and Wang, Zijie J. and Das, Nilaksh and Paul, Anindya S. and Perumalla, Pruthvi and Zhou, Zhiyan and Chau, Duen Horng},
  booktitle={AAAI, Demo},
  year={2021}
}</pre> </div></div> </div><div class="svelte-cyrvy1 pub-block"> <div class="svelte-cyrvy1 news-content"><div class="svelte-cyrvy1 pub"><a href=/papers/unmask target=_self><img alt=Thumbnail class=svelte-cyrvy1 src=/images/teasers/unmask.png></a> <div class="svelte-cyrvy1 pub-content"><div class="svelte-cyrvy1 pub-title"><a href=/papers/unmask target=_self rel=prefetch>UnMask: Adversarial Detection and Defense Through Robust Feature Alignment </a></div> <div class="svelte-cyrvy1 pub-author"><a href=https://www.scottfreitas.com/ target=_self>Scott Freitas</a>, <a href=https://www.cc.gatech.edu/~schen351/ target=_self>Shang-Tse Chen</a>, <a href=https://zijie.wang target=_self style=font-weight:700>Zijie J. Wang</a>, <a href=https://www.cc.gatech.edu/~dchau target=_self>Duen Horng (Polo) Chau</a></div> <div class="svelte-cyrvy1 pub-venue"><a href=https://bigdataieee.org/BigData2020/ target=_self>IEEE International Conference on Big Data (BigData), 2020 </a></div> <div class="svelte-cyrvy1 pub-icons"><a href=/papers/unmask target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#home-sharp></use></svg></div> <span class=svelte-cyrvy1>Project</span> </a> <a href=https://arxiv.org/abs/2002.09576 target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-pdf-solid></use></svg></div> <span class=svelte-cyrvy1>PDF</span> </a> <div class="svelte-cyrvy1 icon-container"><a href=https://github.com/unmaskd/unmask target=_self class="svelte-cyrvy1 icon-container no-right-margin"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> </div> <div class="svelte-cyrvy1 icon-container bibtex-button"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#quote-right-solid></use></svg></div> <span class=svelte-cyrvy1>BibTeX</span> </div> </div> </div></div> <div class="svelte-cyrvy1 bibtex hidden"><pre class=svelte-cyrvy1>@article{freitasUnMaskAdversarialDetection2020,
  title = {{{UnMask}}: {{Adversarial Detection}} and {{Defense Through Robust Feature Alignment}}},
  shorttitle = {{{UnMask}}},
  author = {Freitas, Scott and Chen, Shang-Tse and Wang, Zijie J. and Chau, Duen Horng},
  year = {2020},
  archivePrefix = {arXiv},
  eprint = {2002.09576},
  eprinttype = {arxiv},
  journal = {arXiv:2002.09576}
}</pre> </div></div> </div><div class="svelte-cyrvy1 pub-block pub-block-last"> <div class="svelte-cyrvy1 news-content"><div class="svelte-cyrvy1 pub"><a href=/papers/ai-guideline target=_self><img alt=Thumbnail class=svelte-cyrvy1 src=/images/teasers/ai-guideline.png></a> <div class="svelte-cyrvy1 pub-content"><div class="svelte-cyrvy1 pub-title"><a href=/papers/ai-guideline target=_self rel=prefetch>A Comparative Analysis of Industry Human-AI Interaction Guidelines </a></div> <div class="svelte-cyrvy1 pub-author"><a href=https://austinpwright.com/ target=_self>Austin P. Wright</a>, <a href=https://zijie.wang target=_self style=font-weight:700>Zijie J. Wang</a>, <a href=https://haekyu.com target=_self>Haekyu Park</a>, <a href=https://gracegsy.github.io/ target=_self>Grace Guo</a>, <a href=https://www.vis.uni-konstanz.de/mitglieder/sperrle/ target=_self>Fabian Sperrle</a>, <a href=https://el-assady.com/ target=_self>Mennatallah El-Assady</a>, <a href=https://va.gatech.edu/endert/ target=_self>Alex Endert</a>, <a href=https://www.vis.uni-konstanz.de/en/members/keim target=_self>Daniel Keim</a>, <a href=https://www.cc.gatech.edu/~dchau target=_self>Duen Horng (Polo) Chau</a></div> <div class="svelte-cyrvy1 pub-venue"><a href=https://trexvis.github.io/Workshop2020/ target=_self>IEEE Visualization Conference, Workshop on Trust and Expertise in Visual Analytics (TREX), 2020 </a></div> <div class="svelte-cyrvy1 pub-icons"><a href=/papers/ai-guideline target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#home-sharp></use></svg></div> <span class=svelte-cyrvy1>Project</span> </a> <a href=https://ai-open-guidelines.readthedocs.io/en/latest/ target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#rocket-sharp></use></svg></div> <span class=svelte-cyrvy1>Demo</span> </a> <a href=https://arxiv.org/abs/2010.11761 target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-pdf-solid></use></svg></div> <span class=svelte-cyrvy1>PDF</span> </a> <div class="svelte-cyrvy1 icon-container"><a href=https://github.com/APWright/AI-Open-Guidelines target=_self class="svelte-cyrvy1 icon-container no-right-margin"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> </div> <div class="svelte-cyrvy1 icon-container bibtex-button"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#quote-right-solid></use></svg></div> <span class=svelte-cyrvy1>BibTeX</span> </div> </div> </div></div> <div class="svelte-cyrvy1 bibtex hidden"><pre class=svelte-cyrvy1>@article{wrightComparativeAnalysisIndustry2020,
  title = {A {{Comparative Analysis}} of {{Industry Human}}-{{AI Interaction Guidelines}}},
  author = {Wright, Austin P. and Wang, Zijie J. and Park, Haekyu and Guo, Grace and Sperrle, Fabian and {El-Assady}, Mennatallah and Endert, Alex and Keim, Daniel and Chau, Duen Horng},
  year = {2020},
  month = oct,
  eprint = {2010.11761},
  journal = {arXiv:2010.11761}
}</pre> </div></div> <a href=/cv/#publications target=_self style=padding:0 rel=prefetch><div class="svelte-cyrvy1 svg-icon right-margin show-all-button"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#external-link-alt-solid></use></svg> see all publications </div> </a> </div> <div class="svelte-cyrvy1 project"><div class="svelte-cyrvy1 block-name">Fun Projects</div> <div class="svelte-cyrvy1 project-card"><a href=https://github.com/xiaohk/clip2imgur><img alt=project-teaser class=svelte-cyrvy1 src=/images/teasers/project-clip2imgur.png></a> <div class="svelte-cyrvy1 project-description"><div class="svelte-cyrvy1 project-text"><span class="svelte-cyrvy1 project-name">Clip2imgur:</span> <span class="svelte-cyrvy1 project-detail">Convenient macOS command line tool for uploading screen-shots from the clipboard to Imgur. </span></div> <div class="svelte-cyrvy1 pub-icons"><a href=https://github.com/xiaohk/clip2imgur target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> <div class="svelte-cyrvy1 icon-container award"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#award-solid></use></svg></div> <div class="svelte-cyrvy1 award-highlight"><a href=https://help.imgur.com/hc/en-us/articles/209592766-Tools-for-Imgur target=_self>Featured on Imgur</a></div> </div> </div></div> </div><div class="svelte-cyrvy1 project-card"><a href=https://github.com/xiaohk/FaceData><img alt=project-teaser class=svelte-cyrvy1 src=/images/teasers/project-face.jpg></a> <div class="svelte-cyrvy1 project-description"><div class="svelte-cyrvy1 project-text"><span class="svelte-cyrvy1 project-name">FaceData:</span> <span class="svelte-cyrvy1 project-detail">MacOS GUI to auto-annotate facial landmarks from a video. Landmarks can be used to train GANs. </span></div> <div class="svelte-cyrvy1 pub-icons"><a href=https://github.com/xiaohk/FaceData target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> <div class="svelte-cyrvy1 star-container" style=margin-left:-.5rem><div style=display:flex>(<a href=https://github.com/xiaohk/FaceData/stargazers target=_self class="svelte-cyrvy1 svg-icon" style=font-weight:500;margin-right:-3px> loading </a>)</div> </div> </div></div> </div><div class="svelte-cyrvy1 project-card"><a href=https://github.com/xiaohk/CS559-computational-graphics><img alt=project-teaser class=svelte-cyrvy1 src=/images/teasers/project-graphics.png></a> <div class="svelte-cyrvy1 project-description"><div class="svelte-cyrvy1 project-text"><span class="svelte-cyrvy1 project-name">Graphics on the Web:</span> <span class="svelte-cyrvy1 project-detail">Interactive 2D, 2.5D and 3D computational graphics with shaders and textures, created with HTML canvas and webGL. </span></div> <div class="svelte-cyrvy1 pub-icons"><a href=https://github.com/xiaohk/CS559-computational-graphics target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> <a href=http://jayw-www.cs.wisc.edu/cs559/p10/ target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#rocket-sharp></use></svg></div> <span class=svelte-cyrvy1>Demo</span> </a> </div></div> </div><div class="svelte-cyrvy1 project-card"><a href=https://github.com/xiaohk/CS559-computational-graphics><img alt=project-teaser class=svelte-cyrvy1 src=/images/teasers/project-optim.png></a> <div class="svelte-cyrvy1 project-description"><div class="svelte-cyrvy1 project-text"><span class="svelte-cyrvy1 project-name">Group Assignment Problem:</span> <span class="svelte-cyrvy1 project-detail">Flexible and robust Mixed Integer Quadratic Programming model written in Julia to solve a real-life optimization problem. </span></div> <div class="svelte-cyrvy1 pub-icons"><a href=https://github.com/xiaohk/CS559-computational-graphics target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> <a href=https://nbviewer.jupyter.org/github/xiaohk/CS524-Group-Assignment-Optimization/blob/master/Wang.ipynb target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#rocket-sharp></use></svg></div> <span class=svelte-cyrvy1>Demo</span> </a> <div class="svelte-cyrvy1 icon-container award"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#award-solid></use></svg></div> <div class="svelte-cyrvy1 award-highlight"><a href=https://web.archive.org/web/20200917025916/https://laurentlessard.com/teaching/524-intro-to-optimization/ target=_self>Best project</a></div> </div> </div></div> </div><div class="svelte-cyrvy1 project-card"><a href=https://github.com/xiaohk/d3-china-map><img alt=project-teaser class=svelte-cyrvy1 src=/images/teasers/project-map.png></a> <div class="svelte-cyrvy1 project-description"><div class="svelte-cyrvy1 project-text"><span class="svelte-cyrvy1 project-name">Dean's list Vis:</span> <span class="svelte-cyrvy1 project-detail">Interactive geo-visualization to explore where UW–Madison Chinese students are from. </span></div> <div class="svelte-cyrvy1 pub-icons"><a href=https://github.com/xiaohk/d3-china-map target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> <a href=http://jayw-www.cs.wisc.edu/d3-china-map/ target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#rocket-sharp></use></svg></div> <span class=svelte-cyrvy1>Demo</span> </a> </div></div> </div><div class="svelte-cyrvy1 project-card"><a href=https://github.com/xiaohk/stat333_project_2><img alt=project-teaser class=svelte-cyrvy1 src=/images/teasers/project-review.png></a> <div class="svelte-cyrvy1 project-description"><div class="svelte-cyrvy1 project-text"><span class="svelte-cyrvy1 project-name">Yelp Sentiment:</span> <span class="svelte-cyrvy1 project-detail">Predicting Yelp ratings based on text comments of restaurants at Madison Wisconsin. </span></div> <div class="svelte-cyrvy1 pub-icons"><a href=https://github.com/xiaohk/stat333_project_2 target=_self class="svelte-cyrvy1 icon-container"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></div> <span class=svelte-cyrvy1>Code</span></a> <div class="svelte-cyrvy1 icon-container award"><div class="svelte-cyrvy1 svg-icon"><svg class=svelte-cyrvy1 viewBox="0 0 100 100"><use xlink:href=/sprite.svg#award-solid></use></svg></div> <div class="svelte-cyrvy1 award-highlight"><a href=https://www.kaggle.com/c/uw-madison-sp17-stat333 target=_self>Kaggle winner</a></div> </div> </div></div> </div> </div></div></div> <div class="svelte-cyrvy1 right-padding"></div></div></main> <footer class=svelte-1pltrfa><div class=left-padding></div> <div class="svelte-1pltrfa footer-main"><div class="svelte-1pltrfa footer"><div class="svelte-1pltrfa footer-item"><span>Designed and built by <a href=. class="svelte-1pltrfa raleway">Jay Wang</a> with</span> <div class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#t-heart></use></svg></div> <span>using <a href=https://svelte.dev/ target=_self class=svelte-1pltrfa>Svelte</a> and <a href=https://sapper.svelte.dev/ target=_self class=svelte-1pltrfa>Sapper</a>.</span></div> <div class="svelte-1pltrfa footer-other"><div class=footer-icons><a href=cv target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#file-alt-regular></use></svg></a> <a href=https://github.com/xiaohk target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#github-alt-brands></use></svg></a> <a href="https://scholar.google.com/citations?user=eouAYvcAAAAJ&hl=en" target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#google-scholar></use></svg></a> <a href=http://twitter.com/jay4w target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#twitter-brands></use></svg></a> <a href=https://www.linkedin.com/in/zijiewang/ target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#linkedin-brands></use></svg></a> <a href=mailto:jayw@gatech.edu target=_self class="svg-icon svelte-1pltrfa"><svg class=svelte-1pltrfa viewBox="0 0 100 100"><use xlink:href=/sprite.svg#envelope-regular></use></svg></a></div> <div>© 2020 <a href=. class="svelte-1pltrfa raleway">Zijie Jay Wang</a></div></div></div></div> <div class=right-padding></div></footer></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,(function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am){return {data:{people:{"Zijie J. Wang":{url:"https:\u002F\u002Fzijie.wang",isMe:c},"Anthony Gitter":{url:"https:\u002F\u002Fwww.biostat.wisc.edu\u002F~gitter"},"Melissa C. Skala":{url:"https:\u002F\u002Fmorgridge.org\u002Fresearch\u002Fmedical-engineering\u002Foptical-microscopy"},"Alex J. Walsh":{url:"https:\u002F\u002Fqoil.engr.tamu.edu"},"Duen Horng (Polo) Chau":{url:F},"Polo Chau":{url:F},"Fred Hohman":{url:"https:\u002F\u002Ffredhohman.com"},"Minsuk Kahng":{url:"https:\u002F\u002Fminsuk.com"},"Haekyu Park":{url:"https:\u002F\u002Fhaekyu.com"},"Nilaksh Das":{url:"https:\u002F\u002Fnilakshdas.com"},"Robert Turko":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Frobert-turko\u002F"},"Omar Shaikh":{url:"http:\u002F\u002Foshaikh.com\u002F"},"Michael Gleicher":{url:G},"Yu Hen Hu":{url:"http:\u002F\u002Fhomepages.cae.wisc.edu\u002F~hu\u002F"},"Tiffany M. Heaster":{url:"https:\u002F\u002Fmorgridge.org\u002Fprofile\u002Ftiffany-heaster\u002F"},"Quan Yin":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fquan-yin\u002F"},"Emily Rogers":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Femily-rogers-1a828598"},"Robert Firstman":{url:"https:\u002F\u002Fwww.robfirstman.com\u002F"},"Scott Freitas":{url:"https:\u002F\u002Fwww.scottfreitas.com\u002F"},"Shang-Tse Chen":{url:"https:\u002F\u002Fwww.cc.gatech.edu\u002F~schen351\u002F"},"Jon Saad-Falcon":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fjonsaadfalcon\u002F"},"Austin P. Wright":{url:"https:\u002F\u002Faustinpwright.com\u002F"},"Sasha Richardson":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fsasha-richardson\u002F"},"Siwei Li":{url:"https:\u002F\u002Frsli.github.io\u002F"},"Zhiyan Zhou":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Ffrank-zhou-b19515159\u002F"},"Anish Upadhayay":{url:"https:\u002F\u002Fgithub.com\u002Faupadhayay3"},"Susanta Routray":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fsusantaroutray\u002F"},"Matthew Hull":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fmdhull\u002F"},"Liang Gou":{url:H},"Grace Guo":{url:"https:\u002F\u002Fgracegsy.github.io\u002F"},"Fabian Sperrle":{url:"https:\u002F\u002Fwww.vis.uni-konstanz.de\u002Fmitglieder\u002Fsperrle\u002F"},"Mennatallah El-Assady":{url:"https:\u002F\u002Fel-assady.com\u002F"},"Alex Endert":{url:"https:\u002F\u002Fva.gatech.edu\u002Fendert\u002F"},"Daniel Keim":{url:"https:\u002F\u002Fwww.vis.uni-konstanz.de\u002Fen\u002Fmembers\u002Fkeim"},"Anindya S. Paul":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fanindyasankar\u002F"},"Pruthvi Perumalla":{url:"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fpruthvi-perumalla\u002F"}},education:[{school:p,schoolURL:x,place:y,timeStart:j,timeEnd:h,descriptions:["Ph.D. in Machine Learning"],advisors:[a]},{school:e,schoolURL:k,place:g,timeStart:"Sept. 2015",timeEnd:q,descriptions:["Bachelor of Science (B.S.), GPA: 3.95\u002F4.00","Majors: Computer Sciences (Honor), Statistics (Honor), Mathematics"],advisors:[l,r,I],thesis:{title:"Classifying T Cell Activity with Convolutional Neural Networks",file:"pdf\u002Fundergrad-thesis.pdf"}}],experienceAcademic:[{institution:p,place:y,position:"Ph.D. Researcher",group:J,timeStart:j,timeEnd:h,mentors:[a],description:"Member of the Polo Club of Data Science where we innovate scalable, interactive, and interpretable tools that amplify human's ability to understand and interact with billion-scale data and machine learning models.\n",institutionURL:x,groupURL:"https:\u002F\u002Fcse.gatech.eduu"},{institution:"Morgridge Institute for Research",place:g,position:K,group:"John W. and Jeanne M. Rowe Center for Research in Virology",timeStart:L,timeEnd:j,mentors:[l],description:"Classify T-cell and breast cancer cell types using fluorescent images with machine learning classifiers with a gradient of complexity. Interpre feature representations of each classifiers. Analyze about 1 million 5-channel cell-painting images of bone tumor cells. Explore latent space between image space and chemical molecule space.\n",institutionURL:"https:\u002F\u002Fmorgridge.org",groupURL:"https:\u002F\u002Fmorgridge.org\u002Fresearch\u002Fvirology\u002F"},{institution:e,place:g,position:K,group:M,timeStart:N,timeEnd:"June 2019",mentors:[r],description:"Design and develop a visual analytics tool for recommender system resaerchers. Interactively visualized user-item rating matrix with statistics-conditioned sub-sampling to spot abnormal ratings and predictions.\n",institutionURL:k,groupURL:"https:\u002F\u002Fwww.cs.wisc.edu"},{institution:e,place:g,position:"Research Assistant",group:"Electrical & Computer Engineering",timeStart:"Feb. 2017",timeEnd:L,mentors:[I],description:"Study how to track car driver’s head position and orientation from low-qualitytraffic video. Develop semi-automatic video annotation software with Viola-Jones frontal facedetector for training object tracking algorithms. Implement real-time face tracking algorithms on iOS devices. Train a facial reenactment model using GANs and port it to iOS device.\n",institutionURL:k,groupURL:"https:\u002F\u002Fwww.engr.wisc.edu\u002Fdepartment\u002Felectrical-computer-engineering\u002F"}],experienceIndustry:[{institution:O,place:"Sunnyvale, CA",position:"Research Intern",group:P,timeStart:"June 2020",timeEnd:z,mentors:[Q],description:"Research and develop explainable machine learning models and visual analytics solutions to help autonomous driving domain experts understand and control adversarial case generation for object recognition.\n",institutionURL:s,groupURL:s}],publication:[{id:R,title:"SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models",authors:[f,b,i,"Anindya S. Paul","Pruthvi Perumalla",S,a],venue:"AAAI Conference on Artificial Intelligence Demo",venueURL:"https:\u002F\u002Faaai.org\u002FConferences\u002FAAAI-21\u002Faaai21demoscall\u002F",venueShort:"AAAI",location:"Vancouver, Canada",year:m,url:"\u002Fpapers\u002Fskeleton-vis",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2101.10586",repo:"poloclub\u002Fskeleton-vis",showStar:A,video:"https:\u002F\u002Fyoutu.be\u002FxgK9maDqhi4",demo:"https:\u002F\u002Fpoloclub.github.io\u002Fskeleton-vis\u002F",abstract:"Skeleton-based human action recognition technologies are increasingly used\nin video based applications, such as home robotics, healthcare on aging\npopulation, and surveillance. However, such models are vulnerable to\nadversarial attacks, raising serious concerns for their use in\nsafety-critical applications. To develop an effective defense against\nattacks, it is essential to understand how such attacks mislead the pose\ndetection models into making incorrect predictions. We present SkeletonVis,\nthe first interactive system that visualizes how the attacks work on the\nmodels to enhance human understanding of attacks. ",crownCaption:"The interface of SkeletonVis, visualizing how the Fast Gradient Method\nmanipulates the left foot joints detected by the Detectron2 Keypoint R-CNN\nmodel. (A) The Skeleton Views hows the joints perturbed to unexpected\nlocations. (B) Timeline View reveals the attacked joints spuriously jumping\naround from one frame to the next, leading to a “spike” in the average joint\ndisplacement across attacked frames. These manipulations finally sway the\nST-GCN action detection model into misclassifying the attacked frames as\n“exercising with exercise ball,” instead of the correct “lunge”\nclassification.   ",bibtex:"@article{park2021skeletonvis,\n  title={SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models},\n  author={Park, Haekyu and Wang, Zijie J. and Das, Nilaksh and Paul, Anindya S. and Perumalla, Pruthvi and Zhou, Zhiyan and Chau, Duen Horng},\n  booktitle={AAAI, Demo},\n  year={2021}\n}"},{id:T,title:U,authors:[b,B,n,f,i,t,V,a],venue:"IEEE Transactions on Visualization and Computer Graphics",venueURL:"https:\u002F\u002Fwww.computer.org\u002Fcsdl\u002Fjournal\u002Ftg",venueShort:"TVCG",location:u,year:m,url:"\u002Fpapers\u002Fcnn-explainer",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2004.15004",repo:"poloclub\u002Fcnn-explainer",showStar:c,awards:[{name:"Top of GitHub Trending",url:"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20200505121955\u002Fhttps:\u002F\u002Fgithub.com\u002Ftrending"}],video:"https:\u002F\u002Fyoutu.be\u002FHnWIHWFbuUQ",demo:"https:\u002F\u002Fpoloclub.github.io\u002Fcnn-explainer\u002F",abstract:"Deep learning's great success motivates many practitioners and students to\nlearn about this exciting technology. However, it is often challenging for\nbeginners to take their first step due to the complexity of understanding\nand applying deep learning. We present CNN Explainer, an interactive\nvisualization tool designed for non-experts to learn and examine\nconvolutional neural networks (CNNs), a foundational deep learning model\narchitecture. Our tool addresses key challenges that novices face while\nlearning about CNNs, which we identify from interviews with instructors and\na survey with past students. CNN Explainer tightly integrates a model\noverview that summarizes a CNN's structure, and on-demand, dynamic visual\nexplanation views that help users understand the underlying components of\nCNNs. Through smooth transitions across levels of abstraction, our tool\nenables users to inspect the interplay between low-level mathematical\noperations and high-level model structures. A qualitative user study shows\nthat CNN Explainer helps users more easily understand the inner workings of\nCNNs, and is engaging and enjoyable to use. We also derive design lessons\nfrom our study. Developed using modern web technologies, CNN Explainer runs\nlocally in users' web browsers without the need for installation or\nspecialized hardware, broadening the public's education access to modern\ndeep learning techniques.",crownCaption:"With CNN Explainer, learners can visually examine how Convolutional Neural\nNetworks (CNNs) transform input images into classification predictions\n(e.g., predicting espresso for an image of a coffee cup), and interactively\nlearn about their underlying mathematical operations. In this example, a\nlearner uses CNN Explainer to understand how convolutional layers work\nthrough three tightly integrated views, each explaining the convolutional\nprocess in increasing levels of detail. (A) The Overview visualizes a CNN\narchitecture where each neuron is encoded as a square with a heatmap\nrepresenting the neuron’s output, and each edge connects the neuron with its\ncorresponding inputs and outputs. (B) Clicking a neuron reveals how its\nactivations are computed by the previous layer’s neurons, displaying the\noften-overlooked intermediate computation through animations of sliding\nkernels. (C) The Convolutional Interactive Formula View allows users to\ninteractively inspect the underlying mathematics of the dot-product\noperation core to convolution, through hovering the 3×3 kernel over the\ninput, and interactively studying the corresponding output. For clarity,\nvisibility of Overview and annotation text is improved, and the overlay is\nre-positioned.",bibtex:"@article{wangCNNExplainerLearning2021,\n  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},\n  shorttitle = {{{CNN Explainer}}},\n  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  journal={IEEE Transactions on Visualization and Computer Graphics (TVCG)},\n  publisher={IEEE},\n  year={2021},\n}"},{id:W,title:"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks",authors:[i,f,b,t,X,Y,a],equals:[i,f],venue:Z,venueURL:v,venueShort:_,location:u,year:d,url:"\u002Fpapers\u002Fbluff",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2009.02608",repo:"poloclub\u002Fbluff",showStar:A,demo:"https:\u002F\u002Fpoloclub.github.io\u002Fbluff\u002F",abstract:"Deep neural networks (DNNs) are now commonly used in many domains. However,\nthey are vulnerable to adversarial attacks: carefully crafted perturbations\non data inputs that can fool a model into making incorrect predictions.\nDespite significant research on developing DNN attack and defense\ntechniques, people still lack an understanding of how such attacks penetrate\na model's internals. We present Bluff, an interactive system for\nvisualizing, characterizing, and deciphering adversarial attacks on\nvision-based neural networks. Bluff allows people to flexibly visualize and\ncompare the activation pathways for benign and attacked images, revealing\nmechanisms that adversarial attacks employ to inflict harm on a model. Bluff\nis open-sourced and runs in modern web browsers. ",crownCaption:"With Bluff, users interactively visualize how adversarial attacks penetrate\na deep neural network to induce incorrect outcomes. Here, a user inspects\nwhy Inception V1 misclassifies adversarial giant panda images, crafted by\nthe Projected Gradient Descent (PGD) attack, as armadillo. PGD successfully\nperturbed pixels to induce the “brown bird” feature, an appearance more\nlikely shared by an armadillo (small, roundish, brown body) than a panda,\nactivating more features that contribute to the armadillo (mis)classification\n(e.g., “scales,” “bumps,” “mesh”). The adversarial pathways, formed by these\nneurons and their connections, overwhelm the benign panda pathways and lead\nto the ultimate misclassification. (A) Control Side bar allows users to\nspecify what data is to be included and highlighted. (B) Graph Summary View\nvisualizes pathways most activated or changed by an attack as a network\ngraph of neurons (each labeled by the channel ID in its layer) and their\nconnections. When hovering over a neuron, (C) Detail View displays its\nfeature visualization, representative dataset examples, and activation\npatterns over attack strengths.",bibtex:"@article{dasBluffInteractivelyDeciphering2020,\n  title={Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks},\n  author={Das, Nilaksh and Park, Haekyu and Wang, Zijie J and Hohman, Fred and Firstman, Robert and Rogers, Emily and Chau, Duen Horng},\n  booktitle={IEEE Visualization Conference (VIS)},\n  publisher={IEEE},\n  year={2020}\n}"},{id:$,title:"A Comparative Analysis of Industry Human-AI Interaction Guidelines",authors:[aa,b,f,"Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel Keim",a],venue:"IEEE Visualization Conference, Workshop on Trust and Expertise in Visual Analytics",venueURL:"https:\u002F\u002Ftrexvis.github.io\u002FWorkshop2020\u002F",venueShort:"TREX",location:u,year:d,url:"\u002Fpapers\u002Fai-guideline",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2010.11761",repo:"APWright\u002FAI-Open-Guidelines",showStar:A,demo:"https:\u002F\u002Fai-open-guidelines.readthedocs.io\u002Fen\u002Flatest\u002F",crownMaxHeight:"750px",crownShowBorder:c,abstract:"With the recent release of AI interaction guidelines from Apple, Google, and\nMicrosoft, there is clearly interest in understanding the best practices in\nhuman-AI interaction. However, industry standards are not determined by a\nsingle company, but rather by the synthesis of knowledge from the whole\ncommunity. We have surveyed all of the design guidelines from each of these\nmajor companies and developed a single, unified structure of guidelines,\ngiving developers a centralized reference. We have then used this framework\nto compare each of the surveyed companies to find differences in areas of\nemphasis. Finally, we encourage people to contribute additional guidelines\nfrom other companies, academia, or individuals, to provide an open and\nextensible reference of AI design guidelines at \u003Ca\nhref='https:\u002F\u002Fai-open-guidelines.readthedocs.io'\u003Ehttps:\u002F\u002Fai-open-guidelines.readthedocs.io\u003C\u002Fa\u003E.    ",crownCaption:"Unified Guideline Structure. The inner ring consists of the higher level\ncategorizations, and further sub-categorizations developed during the\naffinity diagram process are shown concentrically. The outermost rays\nconsist of the specific guidelines colored based on their\ncategorizations. Further references on each guideline and its corresponding\ncategorization and source document can be found in the appendix.",bibtex:"@article{wrightComparativeAnalysisIndustry2020,\n  title = {A {{Comparative Analysis}} of {{Industry Human}}-{{AI Interaction Guidelines}}},\n  author = {Wright, Austin P. and Wang, Zijie J. and Park, Haekyu and Guo, Grace and Sperrle, Fabian and {El-Assady}, Mennatallah and Endert, Alex and Keim, Daniel and Chau, Duen Horng},\n  year = {2020},\n  month = oct,\n  eprint = {2010.11761},\n  journal = {arXiv:2010.11761}\n}"},{id:"people-map",title:"Mapping Researchers with PeopleMap",authors:[ab,n,b,aa,"Sasha Richardson",a],venue:"Poster, IEEE Visualization Conference",venueURL:v,venueShort:_,year:d,location:u,url:"\u002Fpapers\u002Fpeople-map",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2009.00091",repo:"poloclub\u002Fpeople-map",type:o,demo:"https:\u002F\u002Fpoloclub.github.io\u002Fpeople-map\u002Fideas\u002F",showStar:c,awards:[{name:"Best Poster Research Award, Honorable Mention",url:"https:\u002F\u002Fvirtual.ieeevis.org\u002Fawards.html"}],abstract:"Discovering research expertise at institutions can be a difficult task.\nManually curated university directories easily become out of date and they\noften lack the information necessary for understanding a researcher's\ninterests and past work, making it harder to explore the diversity of\nresearch at an institution and identify research talents. This results in\nlost opportunities for both internal and external entities to discover new\nconnections and nurture research collaboration. To solve this problem, we\nhave developed PeopleMap, the first interactive, open-source, web-based tool\nthat visually \"maps out\" researchers based on their research interests and\npublications by leveraging embeddings generated by natural language\nprocessing (NLP) techniques. PeopleMap provides a new engaging way for\ninstitutions to summarize their research talents and for people to discover\nnew connections. The platform is developed with ease-of-use and\nsustainability in mind. Using only researchers' Google Scholar profiles as\ninput, PeopleMap can be readily adopted by any institution using its\npublicly-accessible repository and detailed documentation.",crownCaption:"PeopleMap visually maps out researchers based on their research interests\nand publications. Here, a PeopleMap user is exploring the research topics of\nthe faculty members at the Institute of Data Engineering and Science (IDEaS)\nat Georgia Tech (https:\u002F\u002Fpoloclub.github.io\u002Fpeople-map\u002Fideas\u002F) A. Map View\nvisualizes the embedding of researchers generated using their research\ntopics and publication data, with each dot representing a researcher. B.\nResearch Query allows users to search for researchers and query areas of\nstudy, allowing the user to both locate specific individuals and see the\nresearchers most associated with a queried field in the Map View. C.\nResearcher View shows the detailed information (e.g., affiliation,\ncitations, interests) of a researcher highlighted in Map View. D. Control\nPanel allows users to adjust the hyperparameters of the Map View\nvisualization (e.g., show research names and cluster information).",bibtex:"@article{saad-falconMappingResearchersPeopleMap2020,\n  title = {Mapping {{Researchers}} with {{PeopleMap}}},\n  author = {{Saad-Falcon}, Jon and Shaikh, Omar and Wang, Zijie J. and Wright, Austin P. and Richardson, Sasha and Chau, Duen Horng},\n  year = {2020},\n  journal = {Poster, IEEE Visualization Conference (VIS)}\n}"},{id:"argo-lite",title:"Argo Lite: Open-Source Interactive Graph Exploration and Visualization in Browsers",authors:["Siwei Li",S,"Anish Upadhayay",n,ac,f,b,"Susanta Routray","Matthew Hull",a],venue:"The Conference on Information and Knowledge Management",venueShort:ad,venueURL:ae,year:d,location:"Galway, Ireland",url:"\u002Fpapers\u002Fargo-lite",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2008.11844",repo:"poloclub\u002Fargo-graph-lite",showStar:c,type:o,demo:"https:\u002F\u002Fpoloclub.github.io\u002Fargo-graph-lite\u002F",abstract:"Graph data have become increasingly common. Visualizing them helps people\nbetter understand relations among entities. Unfortunately, existing graph\nvisualization tools are primarily designed for single-person desktop use,\noffering limited support for interactive web-based exploration and online\ncollaborative analysis. To address these issues, we have developed Argo\nLite, a new in-browser interactive graph exploration and visualization tool.\nArgo Lite enables users to publish and share interactive graph\nvisualizations as URLs and embedded web widgets. Users can explore graphs\nincrementally by adding more related nodes, such as highly cited papers\ncited by or citing a paper of interest in a citation network. Argo Lite\nworks across devices and platforms, leveraging WebGL for high-performance\nrendering. Argo Lite has been used by over 1,000 students at Georgia Tech's\nData and Visual Analytics class. Argo Lite may serve as a valuable\nopen-source tool for advancing multiple CIKM research areas, from data\npresentation, to interfaces for information systems and more.",crownCaption:"Argo Lite visualizing a citation network of recent COVID-19 publications.\nArgo Lite users can explore graphs incrementally by adding more\nrelated papers (e.g., highly cited papers cited by or citing a paper of\ninterest) to the visualization. Using WebGL for high-performance\ncross-platform graph rendering, Argo Lite runs in all modern web\nbrowsers without requiring any installation.",bibtex:"@article{liArgoLiteOpenSource2020,\n  title={Argo Lite: Open-Source Interactive Graph Exploration and Visualization in Browsers},\n  author={Li, Siwei and Zhou, Zhiyan and Upadhayay, Anish and Shaikh, Omar and Freitas, Scott and Park, Haekyu and Wang, Zijie J and Routray, Susanta and Hull, Matthew and Chau, Duen Horng},\n  booktitle={Proceedings of the International Conference on Information and Knowledge Management},\n  year={2020},\n  organization={ACM}\n}"},{id:af,title:"UnMask: Adversarial Detection and Defense Through Robust Feature Alignment",authors:[ac,"Shang-Tse Chen",b,a],venue:"IEEE International Conference on Big Data",venueShort:"BigData",venueURL:"https:\u002F\u002Fbigdataieee.org\u002FBigData2020\u002F",year:d,location:"Los Angeles, CA, USA",url:"\u002Fpapers\u002Funmask",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2002.09576",repo:"unmaskd\u002Funmask",type:o,abstract:"Deep learning models are being integrated into a wide range of high-impact,\nsecurity-critical systems, from self-driving cars to medical diagnosis.\nHowever, recent research has demonstrated that many of these deep learning\narchitectures are vulnerable to adversarial attacks--highlighting the vital\nneed for defensive techniques to detect and mitigate these attacks before\nthey occur. To combat these adversarial attacks, we developed UnMask, an\nadversarial detection and defense framework based on robust feature\nalignment. The core idea behind UnMask is to protect these models by\nverifying that an image's predicted class (\"bird\") contains the expected\nrobust features (e.g., beak, wings, eyes). For example, if an image is\nclassified as \"bird\", but the extracted features are wheel, saddle and\nframe, the model may be under attack. UnMask detects such attacks and\ndefends the model by rectifying the misclassification, re-classifying the\nimage based on its robust features. Our extensive evaluation shows that\nUnMask (1) detects up to 96.75% of attacks, with a false positive rate of\n9.66% and (2) defends the model by correctly classifying up to 93% of\nadversarial images produced by the current strongest attack, Projected\nGradient Descent, in the gray-box setting. UnMask provides significantly\nbetter protection than adversarial training across 8 attack vectors,\naveraging 31.18% higher accuracy. Our proposed method is architecture\nagnostic and fast. We open source the code repository and data with this\npaper: https:\u002F\u002Fgithub.com\u002Funmaskd\u002Funmask. ",crownShowBorder:c,crownCaption:"UnMask Framework Overview. UnMask combats adversarial attacks (in\nred) through extracting robust features from an image (“Bicycle” at top), and\ncomparing them to expected features of the classification (“Bird” at bottom)\nfrom the unprotected model. Low feature overlap signals an\nattack. UnMask rectifies misclassification using the image’s extracted\nfeatures. Our approach detects 96.75% of gray-box attacks (at 9.66% false\npositive rate) and defends the model by correctly classifying up to 93%\nof adversarial images crafted by Projected Gradient Descent (PGD).",bibtex:"@article{freitasUnMaskAdversarialDetection2020,\n  title = {{{UnMask}}: {{Adversarial Detection}} and {{Defense Through Robust Feature Alignment}}},\n  shorttitle = {{{UnMask}}},\n  author = {Freitas, Scott and Chen, Shang-Tse and Wang, Zijie J. and Chau, Duen Horng},\n  year = {2020},\n  archivePrefix = {arXiv},\n  eprint = {2002.09576},\n  eprinttype = {arxiv},\n  journal = {arXiv:2002.09576}\n}"},{id:"massif",title:"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning",authors:[i,f,b,t,X,Y,a],equals:[i,f],venue:ag,venueURL:"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3334480.3382977",venueShort:ah,location:ai,year:d,url:"\u002Fpapers\u002Fmassif",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2001.07769",type:o,abstract:"Deep neural networks (DNNs) are increasingly powering high-stakes\napplications such as autonomous cars and healthcare; however, DNNs are often\ntreated as \"black boxes\" in such applications. Recent research has also\nrevealed that DNNs are highly vulnerable to adversarial attacks, raising\nserious concerns over deploying DNNs in the real world. To overcome these\ndeficiencies, we are developing Massif, an interactive tool for deciphering\nadversarial attacks. Massif identifies and interactively visualizes neurons\nand their connections inside a DNN that are strongly activated or suppressed\nby an adversarial attack. Massif provides both a high-level, interpretable\noverview of the effect of an attack on a DNN, and a low-level, detailed\ndescription of the affected neurons. Massif's tightly coupled views help\npeople better understand which input features are most vulnerable and\nimportant for correct predictions.",crownCaption:"The MASSIF interface. A user Hailey is studying the targeted Fast Gradient\nMethod (FGM) attack performed on the InceptionV1 model. Using the control\npanel (A), she selects “giant panda” as the benign class and “armadillo” as\nthe attack target class. MASSIF generates an attribution graph (B), which\nshows Hailey the neurons within the network that are suppressed in the\nattacked images (B1, blue), shared by both benign and attacked images (B2,\npurple), and emphasized only in the attacked images (B3, orange). Each\nneuron is represented by a node and its feature visualization (C). Hovering\nover any neuron displays example dataset patches that maximally activate\nthe neuron, providing stronger evidence for what a neuron has learned to\ndetect. Hovering over a neuron also highlights its most influential\nconnections from the previous layer (D), allowing Hailey to determine where\nin the network the prediction diverges from the benign class to the attacked\nclass.",bibtex:"@inproceedings{das2020massif,\n  title={Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning},\n  author={Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  booktitle={Proceedings of the 2020 CHI Conference Extended Abstracts on Human Factors in Computing Systems},\n  publisher={ACM},\n  year={2020}\n}"},{id:"cnn-101",title:"CNN 101: Interactive Visual Learning for Convolutional Neural Networks",authors:[b,B,n,f,i,t,V,a],venue:ag,venueShort:ah,location:ai,venueURL:"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3334480.3382899",year:d,url:"\u002Fpapers\u002Fcnn-101",pdf:"https:\u002F\u002Farxiv.org\u002Fabs\u002F2001.02004",type:o,video:"https:\u002F\u002Fyoutu.be\u002Fg082-zitM7s",crownCaption:"The Overview (A) visualizes activation maps of all neurons as heatmaps\nconnected with edges. When user clicks a convolutional neuron in (A), the\nview transitions to the Convolutional Intermediate View (A=\u003EB).The Flatten\nIntermediate View appears when an output neuron is selected instead\n(A=\u003EC). (B) demonstrates the relationship between selected convolutional\nneuron and its previous layer. (B) transitions to the Detail View which illustrates\nthe convolution operation on selected input neuron (B=\u003ED). (C) explains\nthe flatten layer between the second last layer and output layer.",abstract:"The success of deep learning solving previously-thought hard problems has\ninspired many non-experts to learn and understand this exciting technology.\nHowever, it is often challenging for learners to take the first steps due to\nthe complexity of deep learning models. We present our ongoing work, CNN\n101, an interactive visualization system for explaining and teaching\nconvolutional neural networks. Through tightly integrated interactive views,\nCNN 101 offers both overview and detailed descriptions of how a model works.\nBuilt using modern web technologies, CNN 101 runs locally in users' web\nbrowsers without requiring specialized hardware, broadening the public's\neducation access to modern deep learning techniques. ",bibtex:"@inproceedings{wangCNN101Interactive2020,\n  title = {{{CNN}} 101: {{Interactive}} Visual Learning for Convolutional Neural Networks},\n  booktitle = {Extended Abstracts of the 2020 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},\n  year = {2020},\n  publisher = {{ACM}},\n  place = {{Honolulu, HI, USA}}\n}"},{id:"t-cell",title:"Classifying T cell activity in autofluorescence intensity images with convolutional neural networks",authors:[b,C,D,l],venue:"Journal of Biophotonics",venueShort:"J. Biophotonics",venueURL:"https:\u002F\u002Fonlinelibrary.wiley.com\u002Fjournal\u002F18640648",year:aj,url:"\u002Fpapers\u002Ft-cell",pdf:"https:\u002F\u002Fonlinelibrary.wiley.com\u002Fdoi\u002Fepdf\u002F10.1002\u002Fjbio.201960050",slides:"\u002Fslides\u002Fhonor_thesis_symposium_2019.pdf",repo:"gitter-lab\u002Ft-cell-classification",showStar:"flase",data:"https:\u002F\u002Fdoi.org\u002F10.5281\u002Fzenodo.2640835",type:"journal",crownShowBorder:c,crownCaption:"Our T cell image data processing workflow.",abstract:"The importance of T cells in immunotherapy has motivated developing\ntechnologies to better characterize T cells and improve therapeutic\nefficacy. One specific objective is assessing antigen-induced T cell\nactivation because only functionally active T cells are capable of killing\nthe desired targets. Autofluorescence imaging can distinguish T cell\nactivity states of individual cells in a non-destructive manner by detecting\nendogenous changes in metabolic co-enzymes such as NAD(P)H. However,\nrecognizing robust patterns of T cell activity is computationally\nchallenging in the absence of exogenous labels or information-rich\nautofluorescence lifetime measurements. We demonstrate that advanced machine\nlearning can accurately classify T cell activity from NAD(P)H intensity\nimages and that those image-based signatures transfer across human donors.\nUsing a dataset of 8,260 cropped single-cell images from six donors, we\nmeticulously evaluate multiple machine learning models. These range from\ntraditional models that represent images using summary statistics or extract\nimage features with CellProfiler to deep convolutional neural networks\n(CNNs) pre-trained on general non-biological images. Adapting pre-trained\nCNNs for the T cell activity classification task provides substantially\nbetter performance than traditional models or a simple CNN trained with the\nautofluorescence images alone. Visualizing the images with dimension\nreduction provides intuition into why the CNNs achieve higher accuracy than\nother approaches. However, we observe that fine-tuning all layers of the\npre-trained CNN does not provide a classification performance boost\ncommensurate with the additional computational cost. Our software detailing\nour image processing and model training pipeline is available as Jupyter\nnotebooks at https:\u002F\u002Fgithub.com\u002Fgitter-lab\u002Ft-cell-classification.",bibtex:"@article{wang_classifying_2019,\n  title = {Classifying {T} cell activity in autofluorescence intensity images with convolutional neural networks},\n  issn = {1864-063X, 1864-0648},\n  url = {https:\u002F\u002Fonlinelibrary.wiley.com\u002Fdoi\u002Fabs\u002F10.1002\u002Fjbio.201960050},\n  doi = {10.1002\u002Fjbio.201960050},\n  language = {en},\n  urldate = {2020-01-12},\n  journal = {Journal of Biophotonics},\n  author = {Wang, Zijie J. and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  month = dec,\n  year = {2019}\n}"},{id:"t-cell-poster",title:ak,authors:[b,C,D,l],venue:"International Society for Computational Biology Great Lakes Bioinformatics Conference",venueShort:"ISCB GLBIO",venueURL:"https:\u002F\u002Fwww.iscb.org\u002Fglbio2019",location:"Madison, WI, USA",year:aj,url:"\u002Fpapers\u002Ft-cell-poster",pdf:"\u002Fpdf\u002F19-tcell-glbio.pdf",code:"https:\u002F\u002Fgithub.com\u002Fgitter-lab\u002Ft-cell-classification",type:al,crownShowBorder:c,crownCaption:"Poster presented at the International Society for Computational Biology\nGreat Lakes Bioinformatics Conference (ISCB GLBIO).",abstract:"T cell activity state is an important component of immunotherapy efficacy in\nclinical cancer treatment. However, current image-based activity profiling\nmethods destroy cells and require exogenous contrast agents, making them\nunsuitable for clinical applications. In this study, we use non-destructive,\nT cell autofluorescence microscopy images to measure NAD(P)H intensity and\nclassify individual T cells as activated or quiescent. We assess five\nmachine learning methods of increasing complexity, ranging from linear\nclassifiers to deep convolutional neural networks pre-trained on generic\nimages. To evaluate these models and determine whether they are accurate\nacross different human T cell donors, we designed a meticulous nested\ncross-validation scheme to tune and test each model. A retrained\nconvolutional neural network, the most advanced model, achieved an average\naccuracy of 91.4% when classifying quiescent and activated T cells.\nImportantly, it gave 98% accuracy on an independent donor that was held out\nuntil all aspects of the training and tuning procedures were finalized. This\nshows that autofluorescence microscopy with a state-of-the-art image\nclassification algorithm is a powerful tool for label-free and\nnon-destructive assessment of T cell activity state, even when only NAD(P)H\nintensity is provided as the input feature. In addition, our high-throughput\nhyperparameter selection results give empirical insights on practical deep\nlearning deployment with microscopy image data. Similarly, the model\ncomparisons examined the tradeoff between performance and model complexity,\nwhich provides alternative methods that are suitable when computing resource\nare limited. We observe that retraining more layers in a pre-trained\nconvolutional neural network does not bring performance improvements that\njustify the high computational costs. Finally, we are preparing all of our\ncode in Jupyter notebooks with reproducible examples of image processing and\nclassification. These comprehensive notebooks serve as an instructional tool\nfor readers who are not familiar with machine learning application on\nmicroscopy images.",bibtex:"@inproceedings{wang_classifying_poster_2019,\n  title = {Classifying {T} cell activity with convolutional neural networks},\n  language = {en},\n  conference = {International Society for Computational Biology Great Lakes Bioinformatics Conference},\n  author = {Wang, Zijie J. and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  year = {2019}\n}"},{id:"breast-poster",title:"Using Transfer Learning to Classify Breast Cancer Cells with Fluorescence Imaging",authors:[b,"Tiffany M. Heaster","Quan Yin",C,D,l],venue:"University of Wisconsin–Madison Undergraduate Symposium","venue-short":"",venueURL:"https:\u002F\u002Fugradsymposium.wisc.edu",year:2018,url:"\u002Fpapers\u002Fbreast-poster",pdf:"\u002Fpdf\u002F18-breast-symposium.pdf",type:al,abstract:"Studying tumor heterogeneity by analyzing protein or gene expression levels\nover thousands of cells is very challenging. In this project, we instead use\na transfer learning approach to classify cancer cell types solely based on\nfluorescence imaging. We used images of two types of breast cancer cell\nlines – MDA-MB-231 and SKBr3 – to partially retrain a deep convolutional\nneural network Inception v3, which was pre-trained on 10 million natural\nimages with over 400 categories. We hypothesize features extracted from\ngeneral pictures by a deep neural network are portable to classify breast\ncancer cell types. The ability to recognize distinct cell types within\ntumors would provide a powerful tool for analyzing clinical samples.",crownCaption:"Poster presented at the University of Wisconsin–Madison Undergraduate Symposium.",crownShowBorder:c,bibtex:"@inproceedings{wang_using_poster_2018,\n  title = {Using Transfer Learning to Classify Breast Cancer Cells with Fluorescence Imaging},\n  language = {en},\n  conference = {University of Wisconsin–Madison Undergraduate Symposium},\n  author = {Wang, Zijie J. and Heaster, Tiffany M. and Yin, Quan and Walsh, Alex J. and Skala, Melissa C. and Gitter, Anthony},\n  year = {2018}\n}"}],talk:[{name:U,events:[{time:"Oct. 2020",place:Z,url:v,video:"https:\u002F\u002Fyoutu.be\u002FM-pUfWMjXhQ?t=3308"},{time:z,place:"Deepkapha LiveAI",url:"https:\u002F\u002Fdeepkapha.ai\u002F",video:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=56bkWcMfN7I"}]},{name:ak,events:[{time:"Nov. 2019",place:"Out in Science, Technology, Engineering, and Mathematics (oSTEM) Conference",url:"https:\u002F\u002Fwww.ostem.org\u002F"},{time:"April 2019",place:"UW–Madison Senior Honors Thesis Symposium",url:"https:\u002F\u002Fhonors.ls.wisc.edu\u002Fwp-content\u002Fuploads\u002Fsites\u002F1038\u002F2019\u002F04\u002FSymposiumSchedule2019email.pdf"}]}],award:[{name:"Bosch Research Gift Funding ($37k)",description:"My visual analytics research during my internship results in $37,000 gift funding from Bosch.",url:"https:\u002F\u002Fwww.bosch.com\u002Fresearch\u002F",time:"Jan. 2021"},{name:"University Book Store Academic Excellence Award ($1k)",description:"An award recognizing undergraduate students who have completed an\noutstanding independent project, such as a senior thesis.",url:"https:\u002F\u002Fawards.advising.wisc.edu\u002Fcampus-wide-award-recipients\u002F2016-university-book-store-award-recipients\u002F",time:q},{name:"Honors Senior Thesis Summer Research Grant ($3k)",description:"A research grant funding students to undertake more demanding and extensive\nsenior thesis research projects.",url:"https:\u002F\u002Fhonors.ls.wisc.edu\u002Fsenior-thesis-summer-research-grants\u002F",time:"June 2018"},{name:"Welton Summer Sophomore Apprenticeship ($2.5k)",description:"A research grant awarded to talented students to participate in actual,\ncutting-edge research.",url:"https:\u002F\u002Fhonors.ls.wisc.edu\u002Fwelton-summer-sophomore-apprenticeships\u002F",time:"June 2017"},{name:"Dean's List",description:"Achieved at least a 3.60 GPA as freshmen and sophomores, a 3.85 GPA as\njuniors and seniors.",url:"https:\u002F\u002Fregistrar.wisc.edu\u002Fdeanslist\u002F",time:"Aug. 2015 – May 2019"}],teaching:[{title:"Graduate Teaching Assistant",school:p,schoolURL:"https:\u002F\u002Fwww.gatech.edu\u002F",course:"Data & Visual Analytics (CSE 6242)",courseURL:"https:\u002F\u002Fpoloclub.github.io\u002Fcse6242-2020fall-online\u002F",instructor:a,timeStart:z,timeEnd:"Dec. 2020",place:y,description:"Lead homework designs for data visualizations, hold weekly office hours, and answer student questions on Piazza. The course had 1277 graduate students enrolled.\n"},{title:"Undergraduate Teaching Assistant",school:e,schoolURL:w,course:"Computer Graphics (CS 559)",courseURL:"https:\u002F\u002Fgraphics.cs.wisc.edu\u002FWP\u002Fcs559-sp2019\u002Foverview\u002F",instructor:r,timeStart:N,timeEnd:q,place:g,description:"Create course notes and weekly assignments, hold weekly office hours, and answer student questions on Piazza. The course had 180 undergraduates enrolled.\n"},{title:"Notetaker",school:e,schoolURL:w,course:"McBurney Disability Resource Center",courseURL:"https:\u002F\u002Fmcburney.wisc.edu\u002F",timeStart:"Sep. 2016",timeEnd:q,place:g,description:"Provide clearly-written math and statistics notes to students with disability, answer course-related questions.\n"},{title:"Academic Coach",school:e,schoolURL:w,course:"Division of Diversity, Equity and Educational Achievement",courseURL:"https:\u002F\u002Fdiversity.wisc.edu\u002Fabout\u002Fabout-ddeea\u002F",timeStart:"Nov. 2016",timeEnd:"May 2017",place:g,description:"Mentor undergraduate students in DDEEA programs for Data Structure course, design two worksheets and provided detailed solutions every week.\n"},{title:"Tutor",school:e,schoolURL:w,course:"Greater University Tutoring Service",courseURL:"https:\u002F\u002Fguts.wisc.edu\u002F",timeStart:"Jan. 2016",timeEnd:"Jan. 2017",place:g,description:"Instruct peers one-on-one in programming and math problems for three hours weekly, lead review sections to help students study for calculus exams.\n"}],mentoring:[{name:ab,timeStart:"May 2020",timeEnd:h,degree:E},{name:B,timeStart:j,timeEnd:h,degree:E,description:"Machine learning and visualization",awards:["PURA Travel Award (2020)"]},{name:n,timeStart:j,timeEnd:h,degree:E,awards:["Outstanding Freshman Award (2020)"]}],service:{review:[{venue:"IEEE Visual Analytics Science and Technology",venueShort:"VAST",url:"http:\u002F\u002Fieeevis.org",years:[{year:d,yearURL:v}]},{venue:"IEEE Pacific Visualization Symposium",venueShort:"PacificVis",url:"https:\u002F\u002Fieeexplore.ieee.org\u002Fxpl\u002Fconhome\u002F1001657\u002Fall-proceedings",years:[{year:m,yearURL:"http:\u002F\u002Fvis.tju.edu.cn\u002Fpvis2021\u002F"}]},{venue:"EG Conference on Visualization",venueShort:"EuroVis",url:"https:\u002F\u002Fwww.eurovis.org\u002F",years:[{year:m,yearURL:"https:\u002F\u002Fconferences.eg.org"}]},{venue:"ACM Conference on Information and Knowledge Management",venueShort:ad,url:"https:\u002F\u002Fdl.acm.org\u002Fconference\u002Fcikm",years:[{year:d,yearURL:ae}]},{venue:"Journal of Open Source Software",venueShort:"JOSS",url:"https:\u002F\u002Fjoss.theoj.org\u002F",years:[{year:d,yearRL:"https:\u002F\u002Fjoss.theoj.org"}]}],pc:[{venue:"ACM Conference on Information Retrieval",venueShort:"SIGIR",url:"https:\u002F\u002Fsigir.org\u002F",years:[{year:m,yearURL:"https:\u002F\u002Fsigir.org\u002Fsigir2021\u002Findex.html"}]}],membership:[{org:"Institute of Electrical and Electronics Engineers",orgShort:"IEEE",orgURL:"https:\u002F\u002Fwww.ieee.org\u002F",timeStart:"July 2019",timeEnd:h},{org:"Association for Computing Machinery",orgShort:"ACM",orgURL:"https:\u002F\u002Fwww.acm.org\u002F",timeStart:"Dec. 2019",timeEnd:h}]},reference:[{name:"Polo Chau",position:"Associate Professor",department:J,departmentURL:"https:\u002F\u002Fcse.gatech.edu\u002F",institution:p,institutionURL:x,url:"https:\u002F\u002Fcc.gatech.edu\u002F~dchau\u002F"},{name:Q,position:"Principal Research Scientist",department:P,departmentURL:s,institution:O,institutionURL:s,url:H},{name:"Anghony Gitter",position:"Assistant Professor",department:"Department of Biostatistics and Medical Informatics",departmentURL:"https:\u002F\u002Fwww.biostat.wisc.edu\u002F",institution:e,institutionURL:k,url:"https:\u002F\u002Fwww.biostat.wisc.edu\u002F~gitter\u002F"},{name:r,position:"Professor",department:M,departmentURL:"https:\u002F\u002Fwww.cs.wisc.edu\u002F",institution:e,institutionURL:k,url:G}],skill:[{group:"Programming",items:["Python","JavaScript","Swift","R","Julia","PyTorch","TensorFlow","Keras","HTML","CSS","LaTeX","SQL","C++","Git"]},{group:"Design",items:["Affinity Designer","Affinity Photo","Final Cut Pro","Sketch","Keynote","Illustrator","Photoshop"]},{group:"HCI",items:["Think-aloud protocol","User Personas","Rapid Paper Prototyping","Affinity Diagraming"]}],news:[{date:"Aug. 29, 2020",news:"Two papers,\n\u003Ca href='papers\u002Fcnn-explainer'\u003ECNN Explainer\u003C\u002Fa\u003E\nand\n\u003Ca href='papers\u002Fbluff'\u003EBluff\u003C\u002Fa\u003E,\nare accepted for IEEE VIS 2020!"},{date:"June 18, 2020",news:"Started my first internship at \u003Ca href='https:\u002F\u002Fwww.bosch.us\u002Four-company\u002Fbosch-in-the-usa\u002Fsunnyvale\u002F' target='_self'\u003EBosch (Sunnyvale)\u003C\u002Fa\u003E , working with \u003Ca href='https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Flianggou\u002F' target='_self'\u003ELiang Gou\u003C\u002Fa\u003E on visual analytics for autonomous driving.\n"},{date:"April 30, 2020",news:"Posted \u003Ca href='https:\u002F\u002Farxiv.org\u002Fabs\u002F2004.15004' target='_self'\u003ECNN Explainer paper\u003C\u002Fa\u003E on arXiv. CNN Explainer an \u003Ca href='http:\u002F\u002Fpoloclub.github.io\u002Fcnn-explainer\u002F' target='_self'\u003Einteractive tool\u003C\u002Fa\u003E that helps beginners learn CNNs. It is also \u003Ca href='https:\u002F\u002Fgithub.com\u002Fpoloclub\u002Fcnn-explainer' target='_self'\u003Eopen-sourced\u003C\u002Fa\u003E on GitHub.\n"},{date:"Jan 15, 2020",news:"Two papers, \u003Ca href='papers\u002Fcnn-101' target='_self'\u003ECNN 101\u003C\u002Fa\u003E and \u003Ca href='papers\u002Fmassif' target='_self'\u003EMassif\u003C\u002Fa\u003E, are accepted for CHI 2020 Late-Breaking Works!\n"},{date:"Nov. 15, 2019",news:"I will present my T Cell Classification \u003Ca href='\u002Fpdf\u002Fglbio_2019.pdf' target='_self'\u003Eposter\u003C\u002Fa\u003E at \u003Ca href='https:\u002F\u002Fostem.org\u002Fpage\u002Fconference-2019' target='_self'\u003EoSTEM'19\u003C\u002Fa\u003E. See you in Detroit (really miss the Midwest winter!). \n"},{date:"Oct. 1, 2019",news:"I will attend \u003Ca href='http:\u002F\u002Fieeevis.org\u002Fyear\u002F2019\u002Fwelcome' target='_self'\u003EVIS'19\u003C\u002Fa\u003E in Vancouver. Come to talk to me :) \n"},{date:"Aug. 15, 2019",news:"Submitted my first \u003Ca href='https:\u002F\u002Fwww.biorxiv.org\u002Fcontent\u002F10.1101\u002F737346v1' target='_self'\u003Epaper\u003C\u002Fa\u003E on bioRxiv. Check it out!\n"},{date:"May. 11, 2019",news:"\u003Ca href='https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20200928184759\u002Fhttps:\u002F\u002Fcommencement.wisc.edu\u002Fcontent\u002Fuploads\u002F2019\u002F05\u002FSaturday-May-11th-2019-Spring-Commencement-Ceremony-1.pdf' target='_self'\u003EI graduated!\u003C\u002Fa\u003E Will always be a proud badger!! 🦡\n"},{date:"April 20, 2019",news:"Honored to receive the 2019 \u003Ca href='https:\u002F\u002Fawards.advising.wisc.edu\u002Fcampus-wide-award-recipients\u002F2016-university-book-store-award-recipients\u002F' target='_self'\u003E University Book Store Academic Excellence Award\u003C\u002Fa\u003E.\n"},{date:"April 10, 2019",news:"Excited to present my T-cell classification project as a poster in     \u003Ca href='https:\u002F\u002Fwww.iscb.org\u002Fglbio2019' target='_self'\u003EGLBIO'19\u003C\u002Fa\u003E.\n"}],featured:[{id:T,featureImg:"\u002Fimages\u002Fteasers\u002Fcnn-explainer.png"},{id:W,featureImg:"\u002Fimages\u002Fteasers\u002Fbluff.png"},{id:R,featureImg:"\u002Fimages\u002Fteasers\u002Fskeleton-vis.png"},{id:af,featureImg:"\u002Fimages\u002Fteasers\u002Funmask.png"},{id:$,featureImg:"\u002Fimages\u002Fteasers\u002Fai-guideline.png"}],project:[{name:"Clip2imgur",repo:"xiaohk\u002Fclip2imgur",teaser:"\u002Fimages\u002Fteasers\u002Fproject-clip2imgur.png",award:{name:"Featured on Imgur",url:"https:\u002F\u002Fhelp.imgur.com\u002Fhc\u002Fen-us\u002Farticles\u002F209592766-Tools-for-Imgur"},description:"Convenient macOS command line tool for uploading screen-shots from the clipboard to Imgur. \n"},{name:"FaceData",repo:"xiaohk\u002FFaceData",showStar:c,teaser:"\u002Fimages\u002Fteasers\u002Fproject-face.jpg",description:"MacOS GUI to auto-annotate facial landmarks from a video. Landmarks can be used to train GANs.\n"},{name:"Graphics on the Web",repo:am,demo:"http:\u002F\u002Fjayw-www.cs.wisc.edu\u002Fcs559\u002Fp10\u002F",teaser:"\u002Fimages\u002Fteasers\u002Fproject-graphics.png",description:"Interactive 2D, 2.5D and 3D computational graphics with shaders and textures, created with HTML canvas and webGL.\n"},{name:"Group Assignment Problem",repo:am,demo:"https:\u002F\u002Fnbviewer.jupyter.org\u002Fgithub\u002Fxiaohk\u002FCS524-Group-Assignment-Optimization\u002Fblob\u002Fmaster\u002FWang.ipynb",teaser:"\u002Fimages\u002Fteasers\u002Fproject-optim.png",award:{name:"Best project",url:"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20200917025916\u002Fhttps:\u002F\u002Flaurentlessard.com\u002Fteaching\u002F524-intro-to-optimization\u002F"},description:"Flexible and robust Mixed Integer Quadratic Programming model written in Julia to solve a real-life optimization problem.\n"},{name:"Dean's list Vis",demo:"http:\u002F\u002Fjayw-www.cs.wisc.edu\u002Fd3-china-map\u002F",repo:"xiaohk\u002Fd3-china-map",teaser:"\u002Fimages\u002Fteasers\u002Fproject-map.png",description:"Interactive geo-visualization to explore where UW–Madison Chinese students are from.\n"},{name:"Yelp Sentiment",repo:"xiaohk\u002Fstat333_project_2",teaser:"\u002Fimages\u002Fteasers\u002Fproject-review.png",award:{name:"Kaggle winner",url:"https:\u002F\u002Fwww.kaggle.com\u002Fc\u002Fuw-madison-sp17-stat333"},description:"Predicting Yelp ratings based on text comments of restaurants at Madison Wisconsin.\n"}]}}}("Duen Horng (Polo) Chau","Zijie J. Wang",true,2020,"University of Wisconsin–Madison","Haekyu Park","Madison, WI","Present","Nilaksh Das","Aug. 2019","https:\u002F\u002Fwww.wisc.edu","Anthony Gitter",2021,"Omar Shaikh","arxiv","Georgia Institute of Technology","May 2019","Michael Gleicher","https:\u002F\u002Fwww.bosch.us\u002Four-company\u002Fbosch-in-the-usa\u002Fsunnyvale\u002F","Fred Hohman","Salt Lake City, UT, USA","http:\u002F\u002Fieeevis.org\u002Fyear\u002F2020\u002Fwelcome","https:\u002F\u002Fwww.wisc.edu\u002F","https:\u002F\u002Fwww.gatech.edu","Atlanta, GA","Aug. 2020",false,"Robert Turko","Alex J. Walsh","Melissa C. Skala","B.S. in Computer Science, Georgia Institute of Technology","https:\u002F\u002Fwww.cc.gatech.edu\u002F~dchau","http:\u002F\u002Fpages.cs.wisc.edu\u002F~gleicher\u002F","https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Flianggou\u002F","Yu Hen Hu","School of Computational Science and Engineering","Undergraduate Researcher","Dec. 2017","Department of Computer Sciences","Dec. 2018","Bosch Research","Human–Machine Interaction","Liang Gou","skeleton-vis","Zhiyan Zhou","cnn-explainer","CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization","Minsuk Kahng","bluff","Robert Firstman","Emily Rogers","IEEE Visualization Conference","VIS","ai-guideline","Austin P. Wright","Jon Saad-Falcon","Scott Freitas","CIKM","https:\u002F\u002Fwww.cikm2020.org\u002F","unmask","Extended Abstracts on ACM Human Factors in Computing Systems","CHI","Honolulu, HI, USA",2019,"Classifying T cell activity with convolutional neural networks","poster","xiaohk\u002FCS559-computational-graphics"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.7ddfe460.js"}catch(e){main="/client/legacy/client.16a7acb6.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 